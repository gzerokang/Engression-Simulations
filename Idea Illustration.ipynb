{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea Illustration (with code)\n",
    "\n",
    "In this notebook, we show the proposed procedure for super-resolution of CFD/4DF data. \n",
    "\n",
    "### Step 0: Convert ensight data to vtk data\n",
    "\n",
    "Before conducting the analysis, we need to convert the ensight data to vtk data. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import vtk\n",
    "import vtk.numpy_interface.dataset_adapter as dsa\n",
    "wdo = dsa.WrapDataObject\n",
    "from tqdm import tqdm\n",
    "\n",
    "wd = r'/data/ANY-011-001-ori/'  ## change here for other data sets\n",
    "ensightFolder = '/'\n",
    "outputFolder = 'ens-vtk'\n",
    "filename = 'ensight.encas'\n",
    "outputBaseName = 'ANY-011-001'  ## change here to match wd\n",
    "\n",
    "wssComponentArrayNames = ['x_wall_shear',\n",
    "                          'y_wall_shear',\n",
    "                          'z_wall_shear']\n",
    "\n",
    "def get_block_names(dataset: vtk.vtkMultiBlockDataSet):\n",
    "    numBlocks = dataset.GetNumberOfBlocks()\n",
    "    blockNames = [None] * numBlocks\n",
    "    for i in range(numBlocks):\n",
    "        blockNames[i] = dataset.GetMetaData(i).Get(\n",
    "                vtk.vtkMultiBlockDataSet.NAME())\n",
    "    return blockNames\n",
    "\n",
    "def split_multiblock_dataset_by_name(dataset: vtk.vtkMultiBlockDataSet, \n",
    "                                     blockNames: list=[]):\n",
    "    if not blockNames:\n",
    "        blockNames = get_block_names(dataset)\n",
    "        \n",
    "    blocks = {}\n",
    "    for i, name in enumerate(blockNames):\n",
    "        blocks[name] = dataset.GetBlock(i)\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "def dataset_surface_filter(dataset: vtk.vtkUnstructuredGrid):\n",
    "    surfaceFilter = vtk.vtkDataSetSurfaceFilter()\n",
    "    surfaceFilter.SetInputData(dataset)\n",
    "    surfaceFilter.Update()\n",
    "    return surfaceFilter.GetOutput()\n",
    "\n",
    "print('Reading ensight case file...')    \n",
    "\n",
    "filepath = os.path.join(wd,  filename)\n",
    "\n",
    "reader = vtk.vtkEnSightGoldBinaryReader()\n",
    "reader.SetCaseFileName(filepath)\n",
    "reader.Update()\n",
    "\n",
    "print('Updating reader...')\n",
    "\n",
    "np.bool = np.bool_\n",
    "timeset = dsa.vtkDataArrayToVTKArray(reader.GetTimeSets().GetItem(0))\n",
    "print('Reading time series...')\n",
    "datasets = []\n",
    "\n",
    "\n",
    "writer = vtk.vtkXMLMultiBlockDataWriter()\n",
    "writer.SetCompressorTypeToZLib()\n",
    "\n",
    "if not outputBaseName:\n",
    "    outputBaseName = os.path.splitext(filename)[0]\n",
    "\n",
    "\n",
    "numTimePoints = timeset.size\n",
    "numDigits = len(str(numTimePoints))\n",
    "\n",
    "for i, ti in enumerate(tqdm(timeset)):\n",
    "    reader.SetTimeValue(ti)\n",
    "    reader.Update()\n",
    "    dataset = reader.GetOutput()\n",
    "      \n",
    "    \n",
    "    dataset_dsa = wdo(dataset)\n",
    "    dataset_dsa.FieldData.append(ti, 'Time')\n",
    "    \n",
    "    indexStr = f'{i}'.zfill(numDigits)\n",
    "    outputFileName = f'{outputBaseName}_{indexStr}.vtm'\n",
    "    outputFilePath = os.path.join(wd, outputFolder, outputFileName)\n",
    "    \n",
    "    writer.SetInputData(dataset)\n",
    "    writer.SetFileName(outputFilePath)\n",
    "    \n",
    "    writer.Update()\n",
    "\n",
    "timeFileName = f'{outputBaseName}_time.timeset'\n",
    "with open(os.path.join(wd, outputFolder, timeFileName), 'w') as f:\n",
    "    for ti in timeset:\n",
    "        f.write(f'{ti}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, we obtain a folder named \"ens-vtk\" in the working directory. We should do this to all the 10 patients' original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Obtain data from vtk files\n",
    "\n",
    "The next step is to obtain the data we will use in the analysis step from the vtk files. Here is an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vtk\n",
    "import numpy as np\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "\n",
    "reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "reader.SetFileName(\"/data/ANY-011-001-ori/ens-vtk/ANY-011-001_00.vtm\")  ## change here for other data sets\n",
    "reader.Update()\n",
    "\n",
    "## extract data from vtk file\n",
    "\n",
    "def vtk_to_numpy_array(vtk_array):\n",
    "    return vtk.util.numpy_support.vtk_to_numpy(vtk_array)\n",
    "\n",
    "# Extract blocks\n",
    "blocks = [reader.GetOutput().GetBlock(i) for i in range(reader.GetOutput().GetNumberOfBlocks())]\n",
    "\n",
    "# Initialize lists to store data\n",
    "points_list = []\n",
    "point_data_list = []\n",
    "\n",
    "for block in blocks:\n",
    "    if isinstance(block, vtk.vtkUnstructuredGrid):\n",
    "        points = vtk_to_numpy_array(block.GetPoints().GetData())\n",
    "        point_data = vtk_to_numpy_array(block.GetPointData().GetArray('velocity'))\n",
    "\n",
    "        points_list.append(points)\n",
    "        point_data_list.append(point_data)\n",
    "\n",
    "# Convert lists to single numpy arrays\n",
    "all_points = np.vstack(points_list)\n",
    "all_point_data = np.vstack(point_data_list)\n",
    "\n",
    "#np.savez('ANY-011-001_00.npz', all_points=all_points, all_points_data=all_point_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- The data we will use in each vtm file are: (i) all_points, which stores the spatial coordinates $(x,y,z)$ of each measurement. (ii) all_point_data, which stores the velocities $(v_x,v_y,v_z)$ at each measured point. \n",
    "\n",
    "- Each vtk folder contains multiple vtm files, here we only deal with one. We propose to retrieve all_points and all_point_data for all the vtm files, and average them. By doing this, we will have one single file for each patient. \n",
    "\n",
    "- Based on personal experience, installing the python vtk package is a little painful. It requires a specific python version and seems not supporting the latest one. Therefore, I recomend doing this step on a local computer, save the results and then upload to the server. It would not take too long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Processing data\n",
    "\n",
    "In this step, we create image-like data for training and testing. The procedure can be described as follows:\n",
    "\n",
    "- For each measurement, we obtain a local $\\epsilon$-ball of it, based on its Euclidean distance with other measurements. \n",
    "\n",
    "- Depending on how many measurements are contained in the local $\\epsilon$-ball, we can calculate a resolution parameter $t$, such that a larger value means higher resolution. Specifically, $t=\\lfloor\\frac{\\log_2(N)}{3} \\rfloor$, where $N$ is the number of measurements in a local $\\epsilon$-ball. \n",
    "\n",
    "- Then, we create $2^t\\times 2^t\\times 2^t$ voxels. The channel values cooresponding to each voxel are the velocities. For all the measurements in a voxel, we impute a function over the cube by inverse distance weighting. The channel values of the voxel is the function evaluated at the center of the cube. We normalize the velocities such that the value is in between -1 to 1. \n",
    "\n",
    "- To make the tensor having the same size, we mannually extend the voxel size to be $2^T\\times 2^T\\times 2^T$, where $T$ is the highest resolution. The channel vaules of the extended voxels are imputed by nearest neighbor. \n",
    "\n",
    "By doing this, we will obtain $n$ $3\\times 2^T\\times 2^T\\times 2^T$ tensors, where $n$ is the total number of measurements in a vtm file. Each $3\\times 2^T\\times 2^T\\times 2^T$ represents a local \"image\" of the corresponding data point, associated with a resolution parameter $t$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.ndimage import zoom  # For resampling\n",
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm  \n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "def process_point(args):\n",
    "    idx, all_data, all_data_point, epsilon, tree, fixed_grid_size = args\n",
    "    point = all_data[idx]\n",
    "    indices = tree.query_ball_point(point, r=epsilon)\n",
    "    if idx not in indices:\n",
    "        indices.append(idx)\n",
    "    selected_points = all_data[indices]\n",
    "    num_neighbors = len(indices)\n",
    "    t = max(1, int(math.log2(num_neighbors) // 3))\n",
    "\n",
    "    x_min, y_min, z_min = selected_points.min(axis=0)\n",
    "    x_max, y_max, z_max = selected_points.max(axis=0)\n",
    "\n",
    "    num_partitions = 2 ** t\n",
    "    x_edges = np.linspace(x_min, x_max, num_partitions + 1)\n",
    "    y_edges = np.linspace(y_min, y_max, num_partitions + 1)\n",
    "    z_edges = np.linspace(z_min, z_max, num_partitions + 1)\n",
    "    x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "    y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "    z_centers = (z_edges[:-1] + z_edges[1:]) / 2\n",
    "    Xc, Yc, Zc = np.meshgrid(\n",
    "        x_centers, y_centers, z_centers, indexing='ij'\n",
    "    )\n",
    "    centers = np.column_stack((Xc.ravel(), Yc.ravel(), Zc.ravel()))\n",
    "        \n",
    "    channel_x = np.zeros(centers.shape[0])\n",
    "    channel_y = np.zeros(centers.shape[0])\n",
    "    channel_z = np.zeros(centers.shape[0])\n",
    "    \n",
    "    ## normalize velocities\n",
    "    selected_values = all_data_point[indices]\n",
    "    min_vals = selected_values.min(axis=0)\n",
    "    max_vals = selected_values.max(axis=0)\n",
    "    ranges = max_vals - min_vals\n",
    "    ranges[ranges == 0] = 1\n",
    "    selected_val_scaled = (selected_values - min_vals) / ranges\n",
    "    selected_values = selected_val_scaled * 2 - 1\n",
    "\n",
    "    for i, center in enumerate(centers):\n",
    "        distances = np.linalg.norm(selected_points - center, axis=1)\n",
    "        if np.any(distances == 0):\n",
    "            idx_zero = np.where(distances == 0)[0][0]\n",
    "            channel_x[i] = selected_values[idx_zero, 0]\n",
    "            channel_y[i] = selected_values[idx_zero, 1]\n",
    "            channel_z[i] = selected_values[idx_zero, 2]\n",
    "        else:\n",
    "            weights = 1 / distances\n",
    "            weights /= weights.sum()\n",
    "            channel_x[i] = np.dot(weights, selected_values[:, 0])\n",
    "            channel_y[i] = np.dot(weights, selected_values[:, 1])\n",
    "            channel_z[i] = np.dot(weights, selected_values[:, 2])\n",
    "    \n",
    "    channel_values = np.stack((channel_x, channel_y, channel_z), axis=-1)   \n",
    "    tensor_shape = (num_partitions, num_partitions, num_partitions, 3)\n",
    "    tensor = channel_values.reshape(tensor_shape)\n",
    "    \n",
    "    # Resample tensor to fixed grid size using nearest neighbor interpolation\n",
    "\n",
    "    zoom_factors = [fixed_size / float(orig_size) for fixed_size, orig_size in zip(fixed_grid_size, tensor.shape[:3])]\n",
    "    # Apply zoom with order=0 for nearest neighbor interpolation\n",
    "    tensor_resized = zoom(tensor, zoom_factors + [1], order=0)\n",
    "    tensor_resized = torch.from_numpy(tensor_resized).float()\n",
    "\n",
    "    return (tensor_resized, t, idx)\n",
    "\n",
    "def process_all_points_parallel(\n",
    "    all_data, all_data_point, epsilon, batch_size=32, save_dir='tensor_batches', fixed_grid_size=(32, 32, 32)\n",
    "):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    N = all_data.shape[0]\n",
    "    tree = cKDTree(all_data)\n",
    "\n",
    "    args_list = [\n",
    "        (i, all_data, all_data_point, epsilon, tree, fixed_grid_size) for i in range(N)\n",
    "    ]\n",
    "    batch = []\n",
    "    batch_idx = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Use pathos Pool for parallel execution\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        \n",
    "        for result in tqdm(\n",
    "            pool.imap(process_point, args_list), total=N, desc='Processing'\n",
    "        ):\n",
    "            if result is not None:\n",
    "                tensor, t, idx = result\n",
    "                batch.append((tensor, t, idx))\n",
    "                batch_idx += 1\n",
    "                if batch_idx >= batch_size:\n",
    "                    save_path = os.path.join(\n",
    "                        save_dir, f'batch_{num_batches}.pt'\n",
    "                    )\n",
    "                    torch.save(batch, save_path)\n",
    "                    batch = []\n",
    "                    batch_idx = 0\n",
    "                    num_batches += 1\n",
    "\n",
    "    # Save any remaining tensors in the final batch\n",
    "    if batch:\n",
    "        save_path = os.path.join(save_dir, f'batch_{num_batches}.pt')\n",
    "        torch.save(batch, save_path)\n",
    "        num_batches += 1\n",
    "\n",
    "    return num_batches\n",
    "\n",
    "# Define a custom Dataset to load the saved batches\n",
    "class VoxelDataset(Dataset):\n",
    "    def __init__(self, save_dir='tensor_batches'):\n",
    "        self.save_dir = save_dir\n",
    "        self.batch_files = [\n",
    "            os.path.join(save_dir, f)\n",
    "            for f in os.listdir(save_dir)\n",
    "            if f.endswith('.pt')\n",
    "        ]\n",
    "        self.batch_files.sort()\n",
    "        self.index_map = []\n",
    "        self._create_index_map()\n",
    "\n",
    "    def _create_index_map(self):\n",
    "        for batch_file in self.batch_files:\n",
    "            batch = torch.load(batch_file)\n",
    "            batch_size = len(batch)\n",
    "            for i in range(batch_size):\n",
    "                self.index_map.append((batch_file, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_file, tensor_idx = self.index_map[idx]\n",
    "        batch = torch.load(batch_file)\n",
    "        tensor, t, point_idx = batch[tensor_idx]\n",
    "        return tensor, t  # Return both tensor and t\n",
    "\n",
    "# Usage\n",
    "if __name__ == '__main__':\n",
    "    # Assume all_points and all_point_data are defined elsewhere\n",
    "    all_data = all_points\n",
    "    all_data_point = all_point_data\n",
    "    epsilon = 0.001  # Define the epsilon radius\n",
    "    batch_size = 32  # Define the batch size\n",
    "    fixed_grid_size = (16, 16, 16)  # Define the fixed grid size for resampling\n",
    "\n",
    "    # Process all points in parallel and save tensors in batches with progress bar\n",
    "    num_batches = process_all_points_parallel(\n",
    "        all_data, all_data_point, epsilon, batch_size=batch_size, fixed_grid_size=fixed_grid_size\n",
    "    )\n",
    "    print(f\"Saved {num_batches} batches of tensors.\")\n",
    "\n",
    "    # Create a Dataset and DataLoader for PyTorch\n",
    "    dataset = VoxelDataset(save_dir='tensor_batches')\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Iterate over the dataloader\n",
    "    for batch_tensors, batch_t_values in dataloader:\n",
    "        print(f\"Batch tensors shape: {batch_tensors.shape}\")\n",
    "        print(f\"Unique batch t values: {batch_t_values.unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model\n",
    "\n",
    "Now, we train the model. Specificallly, we will learn a function $g$, which improve the resolution of an image by increase its number of voxels by 8 times (2 times in each dimension). \n",
    "\n",
    "- We assume a pre-additive noise model, defined as $\\mathbf{X}_t=g(\\mathbf{X}_{t-1}+\\boldsymbol{\\epsilon}_{t-1})$, where $\\boldsymbol{\\epsilon}_{t-1}\\sim N(0,\\sigma_{t-1}^2\\mathbf{I})$. Currently, we assume $\\sigma_{t-1}^2$ is known. \n",
    "\n",
    "- $\\mathbf{X}_{t-1}$ represents the lower resilution image, and $\\mathbf{X}_t$ represents the higher resolution image. The number of voxels in $\\mathbf{X}_t$ is 8 times the number of voxels in $\\mathbf{X}_{t-1}$. \n",
    "\n",
    "- From the procressing step, we have obtained $\\mathbf{X}_t$ from real data. To obatin its corresponding lower resolution image $\\mathbf{X}_{t-1}$, we downsample $\\mathbf{X}_t$ to half size in each dimension. We repeat this procedure to obtain $\\mathbf{X}_{t-1}$ to $\\mathbf{X}_{0}$.\n",
    "\n",
    "- We use a 3D UNet model to learn the function $g$, using the processed image $\\mathbf{X}_{t-1}$ and generated Gaussian noise $\\boldsymbol{\\epsilon}_{t-1}$ as the input, and $\\mathbf{X}_t$ as the output. Currently, the training process is to minimize the mean squared error loss. \n",
    "\n",
    "- We still want the tensor size to be the same. Therefore, for downsampled image, we use padding to make sure its size is still $3\\times 2^T\\times 2^T\\times 2^T$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.ndimage import zoom  \n",
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm  \n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "       \n",
    "\n",
    "# Define the SuperResolutionDataset\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, images, T, sigma_t_list):\n",
    "        self.images = images\n",
    "        self.T = T\n",
    "        self.sigma_t_list = sigma_t_list\n",
    "        self.data_pairs = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def gaussian_kernel_1d(self, kernel_size, sigma):\n",
    "        # Create a 1D Gaussian kernel\n",
    "        x = torch.arange(kernel_size) - kernel_size // 2\n",
    "        kernel = torch.exp(-0.5 * (x / sigma) ** 2)\n",
    "        kernel = kernel / kernel.sum()\n",
    "        return kernel\n",
    "\n",
    "    def gaussian_blur_3d(self, x, kernel_size=5, sigma=1):\n",
    "        # x: Tensor of shape (C, D, H, W)\n",
    "        device = x.device\n",
    "        x = x.unsqueeze(0)  # Add batch dimension\n",
    "        N, C, D, H, W = x.shape\n",
    "\n",
    "        # Adjust kernel_size if necessary\n",
    "        max_kernel_size = min(kernel_size, D, H, W)\n",
    "        if max_kernel_size % 2 == 0:\n",
    "            max_kernel_size -= 1  # Ensure it's odd\n",
    "        if max_kernel_size < 1:\n",
    "            x_blur = x\n",
    "        else:\n",
    "            # Create 1D Gaussian kernel\n",
    "            kernel = self.gaussian_kernel_1d(max_kernel_size, sigma).to(device)\n",
    "            # Create 3D Gaussian kernel\n",
    "            kernel_3d = kernel[:, None, None] * kernel[None, :, None] * kernel[None, None, :]\n",
    "            kernel_3d = kernel_3d / kernel_3d.sum()\n",
    "            # Reshape kernel for conv3d\n",
    "            kernel_3d = kernel_3d.view(1, 1, max_kernel_size, max_kernel_size, max_kernel_size)\n",
    "            kernel_3d = kernel_3d.repeat(C, 1, 1, 1, 1)\n",
    "            padding = max_kernel_size // 2\n",
    "            # Adjust padding for each dimension\n",
    "            pad_D = min(padding, D - 1)\n",
    "            pad_H = min(padding, H - 1)\n",
    "            pad_W = min(padding, W - 1)\n",
    "            x_padded = F.pad(x, (pad_W, pad_W, pad_H, pad_H, pad_D, pad_D), mode='reflect')\n",
    "            x_blur = F.conv3d(x_padded, kernel_3d, groups=C)\n",
    "        x_blur = x_blur.squeeze(0)  \n",
    "        return x_blur\n",
    "\n",
    "    def interpolate_3d(self, tensor, **kwargs):\n",
    "        # tensor: (C, D, H, W)\n",
    "        tensor = tensor.unsqueeze(0)  \n",
    "        tensor_interp = F.interpolate(tensor, **kwargs)\n",
    "        tensor_interp = tensor_interp.squeeze(0)  \n",
    "        return tensor_interp\n",
    "\n",
    "    def generate_downsampled_images(self, image):\n",
    "        # image: Tensor of shape (C, D, H, W)\n",
    "        # Generate downsampled images from X_T to X_0\n",
    "        X_t_list = []\n",
    "        X_t = image  # Start with the original image at highest resolution\n",
    "        X_t_list.append(X_t)\n",
    "        for t in range(self.T, 0, -1):\n",
    "            # Downsample X_t to half size in each dimension\n",
    "            X_t_down = self.interpolate_3d(X_t, scale_factor=0.5, mode='trilinear', align_corners=False, recompute_scale_factor=True)\n",
    "            # Apply Gaussian blur to X_t_down\n",
    "            X_t_blur = self.gaussian_blur_3d(X_t_down)\n",
    "            # Upsample back to original size\n",
    "            original_size = image.shape[1:]  # (D, H, W)\n",
    "            X_t_blur_upsampled = self.interpolate_3d(X_t_blur, size=original_size, mode='trilinear', align_corners=False)\n",
    "            X_t_list.insert(0, X_t_blur_upsampled)  \n",
    "            X_t = X_t_down  # Update X_t for the next iteration\n",
    "        return X_t_list  # X_t_list[0] corresponds to t=0, X_t_list[T] corresponds to t=T\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Create training pairs (X_{t-1} + ε_{t-1}, X_t) for t = 1 to T\n",
    "        for image in self.images:\n",
    "            X_t_list = self.generate_downsampled_images(image)\n",
    "            for t in range(1, self.T + 1):\n",
    "                X_t = X_t_list[t]\n",
    "                X_t_minus_1 = X_t_list[t - 1]\n",
    "                sigma_t_minus_1 = self.sigma_t_list[t - 1]\n",
    "                \n",
    "                epsilon_t_minus_1 = torch.randn_like(X_t_minus_1) * (sigma_t_minus_1 ** 0.5)\n",
    "                X_t_minus_1_noisy = X_t_minus_1 + epsilon_t_minus_1\n",
    "               \n",
    "                assert X_t_minus_1_noisy.shape == X_t.shape, f\"Shape mismatch: X_t_minus_1_noisy {X_t_minus_1_noisy.shape}, X_t {X_t.shape}\"\n",
    "                self.data_pairs.append((X_t_minus_1_noisy, X_t, t))\n",
    "        # Now self.data_pairs contains all training pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_input, X_target, t = self.data_pairs[idx]\n",
    "        return X_input, X_target, t\n",
    "\n",
    "# Define the 3D UNet model \n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, init_features=32):\n",
    "        super(UNet3D, self).__init__()\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet3D._block(in_channels, features)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2)\n",
    "        self.encoder2 = UNet3D._block(features, features * 2)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2)\n",
    "        self.encoder3 = UNet3D._block(features * 2, features * 4)\n",
    "        \n",
    "\n",
    "        self.bottleneck = UNet3D._block(features * 4, features * 8)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose3d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = UNet3D._block(features * 8, features * 4)\n",
    "        self.upconv2 = nn.ConvTranspose3d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = UNet3D._block(features * 4, features * 2)\n",
    "        self.upconv1 = nn.ConvTranspose3d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = UNet3D._block(features * 2, features)\n",
    "\n",
    "        self.conv = nn.Conv3d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(features, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)  # [N, features, D, H, W]\n",
    "        enc2 = self.encoder2(self.pool1(enc1))  # [N, features*2, D/2, H/2, W/2]\n",
    "        enc3 = self.encoder3(self.pool2(enc2))  # [N, features*4, D/4, H/4, W/4]\n",
    "\n",
    "        bottleneck = self.bottleneck(enc3)  # [N, features*8, D/4, H/4, W/4]\n",
    "\n",
    "        dec3 = self.upconv3(bottleneck)  # [N, features*4, D/2, H/2, W/2]\n",
    "        # Adjust dec3 size if necessary\n",
    "        if dec3.shape[2:] != enc3.shape[2:]:\n",
    "            dec3 = F.interpolate(dec3, size=enc3.shape[2:], mode='trilinear', align_corners=False)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)  # [N, features*2, D, H, W]\n",
    "        if dec2.shape[2:] != enc2.shape[2:]:\n",
    "            dec2 = F.interpolate(dec2, size=enc2.shape[2:], mode='trilinear', align_corners=False)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)  # [N, features, 2D, 2H, 2W]\n",
    "        if dec1.shape[2:] != enc1.shape[2:]:\n",
    "            dec1 = F.interpolate(dec1, size=enc1.shape[2:], mode='trilinear', align_corners=False)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return self.conv(dec1)\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    X_inputs, X_targets, ts = zip(*batch)\n",
    "    # Determine the maximum spatial dimensions in the batch\n",
    "    max_C = max(x_input.shape[0] for x_input in X_inputs)\n",
    "    max_D = max(x_input.shape[1] for x_input in X_inputs)\n",
    "    max_H = max(x_input.shape[2] for x_input in X_inputs)\n",
    "    max_W = max(x_input.shape[3] for x_input in X_inputs)\n",
    "\n",
    "    # Pad all tensors to the maximum size\n",
    "    X_inputs_padded = []\n",
    "    X_targets_padded = []\n",
    "    for x_input, x_target in zip(X_inputs, X_targets):\n",
    "        padding_input = (\n",
    "            0, max_W - x_input.shape[3],  # Width padding\n",
    "            0, max_H - x_input.shape[2],  # Height padding\n",
    "            0, max_D - x_input.shape[1],  # Depth padding\n",
    "        )\n",
    "        padding_target = (\n",
    "            0, max_W - x_target.shape[3],\n",
    "            0, max_H - x_target.shape[2],\n",
    "            0, max_D - x_target.shape[1],\n",
    "        )\n",
    "        x_input_padded = F.pad(x_input, padding_input, mode='constant', value=0)\n",
    "        x_target_padded = F.pad(x_target, padding_target, mode='constant', value=0)\n",
    "        X_inputs_padded.append(x_input_padded)\n",
    "        X_targets_padded.append(x_target_padded)\n",
    "\n",
    "    X_inputs_batch = torch.stack(X_inputs_padded)\n",
    "    X_targets_batch = torch.stack(X_targets_padded)\n",
    "    ts_batch = torch.tensor(ts)\n",
    "    return X_inputs_batch, X_targets_batch, ts_batch\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, criterion, device, num_epochs=10):\n",
    "       model.train()\n",
    "       for epoch in range(num_epochs):\n",
    "           epoch_loss = 0\n",
    "           print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "           \n",
    "           with tqdm(total=len(dataloader), desc=f\"Training Epoch {epoch+1}\", unit=\"batch\") as pbar:\n",
    "               for X_input, X_target, t in dataloader:\n",
    "                   X_input = X_input.to(device)\n",
    "                   X_target = X_target.to(device)\n",
    "                   # Forward pass\n",
    "                   outputs = model(X_input)\n",
    "                   # Ensure outputs and X_target have the same shape\n",
    "                   if outputs.shape != X_target.shape:\n",
    "                       X_target = F.interpolate(X_target, size=outputs.shape[2:], mode='trilinear', align_corners=False)\n",
    "                   loss = criterion(outputs, X_target)\n",
    "                   # Backward pass and optimization\n",
    "                   optimizer.zero_grad()\n",
    "                   loss.backward()\n",
    "                   optimizer.step()\n",
    "                   epoch_loss += loss.item()\n",
    "                   pbar.update(1)\n",
    "           avg_loss = epoch_loss / len(dataloader)\n",
    "           print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "# Main code\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    save_dir = 'tensor_batches'\n",
    "\n",
    "    # List all tensor batch files\n",
    "    batch_files = [\n",
    "        os.path.join(save_dir, f)\n",
    "        for f in os.listdir(save_dir)\n",
    "        if f.endswith('.pt')\n",
    "    ]\n",
    "    batch_files.sort()\n",
    "\n",
    "    images = []\n",
    "\n",
    "    print(\"Loading images from processed tensors...\")\n",
    "    for batch_file in tqdm(batch_files, desc=\"Loading batches\"):\n",
    "        batch = torch.load(batch_file)  # Each batch is a list of (tensor, t, idx)\n",
    "        for data in batch:\n",
    "            tensor, t, idx = data\n",
    "            # tensor is of shape (D, H, W, C), need to permute to (C, D, H, W)\n",
    "            tensor = tensor.permute(3, 0, 1, 2)\n",
    "            images.append(tensor)\n",
    "\n",
    "\n",
    "    T = 4  # Maximum resolution level\n",
    "    sigma_t_list = [0.1] * T  # Prespecified \\sigma_t^2 for each t from 0 to T-1\n",
    "\n",
    "    dataset = SuperResolutionDataset(images, T, sigma_t_list)\n",
    "    batch_size = 32\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    model = UNet3D(in_channels=3, out_channels=3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = 50  # Adjust the number of epochs as needed\n",
    "    train(model, dataloader, optimizer, criterion, device, num_epochs)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = 'unet3d_super_resolution.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to '{model_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Sampling\n",
    "\n",
    "Using the learned super-resolution function $\\hat{g}$, we can improve the resolution of any local image. \n",
    "\n",
    "- For a local image $\\mathbf{X}_t$, we can improve its resolution by 8 times through $\\mathbf{X}_{t+1}=\\hat{g}(\\mathbf{X}_t+\\boldsymbol{\\epsilon}_t)$, where $\\boldsymbol{\\epsilon}_t$ are generated from $N(0,\\sigma_t^2\\mathbf{I})$.  \n",
    "\n",
    "- Repeat this process $T-t$ times, we can obtain the highest resolution image $\\mathbf{X}_T$.\n",
    "\n",
    "- We can use the improved image $\\mathbf{X}_T$ to map back to the original measurements. The general idea is, we should have more information in $\\mathbf{X}_T$ than the original $\\mathbf{X}_t$. Currently, we use the central voxel value of $\\mathbf{X}_T$ to map back to the original measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_super_resolution(model, tensor, t, device, T=4, sigma_t_list=[0.1]*4):\n",
    "    \n",
    "    model.eval()\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    # Permute the tensor dimensions from [D, H, W, C] to [C, D, H, W]\n",
    "    if tensor.dim() == 4:\n",
    "        tensor = tensor.permute(3, 0, 1, 2)\n",
    "    elif tensor.dim() == 5:\n",
    "        tensor = tensor.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "    if t == T:\n",
    "        # Permute back before returning\n",
    "        return tensor.cpu().permute(1, 2, 3, 0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for res_level in range(t, T):\n",
    "                # Add noise ε_t with variance σ_t^2\n",
    "                sigma_t = sigma_t_list[res_level]\n",
    "                epsilon_t = torch.randn_like(tensor) * (sigma_t ** 0.5)\n",
    "                X_input = tensor + epsilon_t\n",
    "\n",
    "                # Apply the model to X_input\n",
    "                X_input = X_input.unsqueeze(0)  # Add batch dimension\n",
    "                tensor = model(X_input).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Permute back to original dimension order before returning\n",
    "        return tensor.cpu().permute(1, 2, 3, 0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the trained model\n",
    "    model_path = 'unet3d_super_resolution.pth'\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
    "    model = UNet3D(in_channels=3, out_channels=3)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Input data directory\n",
    "    input_dir = 'tensor_batches'  # Directory containing your saved data batches\n",
    "    if not os.path.exists(input_dir):\n",
    "        raise FileNotFoundError(f\"Input data directory '{input_dir}' not found.\")\n",
    "\n",
    "    # Output directory\n",
    "    output_dir = 'upsampled_batches'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get list of all .pt files in input_dir\n",
    "    batch_files = [f for f in os.listdir(input_dir) if f.endswith('.pt')]\n",
    "    batch_files.sort()\n",
    "\n",
    "    # Process each batch file\n",
    "    for batch_file in tqdm(batch_files, desc='Processing batches'):\n",
    "        batch_path = os.path.join(input_dir, batch_file)\n",
    "        # Load the batch data\n",
    "        batch_data = torch.load(batch_path)  # Should be a list of (tensor, t, point_idx)\n",
    "\n",
    "        upsampled_batch = []\n",
    "\n",
    "        for data in batch_data:\n",
    "            tensor, t, point_idx = data\n",
    "            # Apply the sampling function\n",
    "            upsampled_tensor = sample_super_resolution(model, tensor, t, device)\n",
    "            # Append the upsampled tensor and point index\n",
    "            upsampled_batch.append((upsampled_tensor, point_idx))\n",
    "\n",
    "        # Save the upsampled batch\n",
    "        output_batch_path = os.path.join(output_dir, batch_file)\n",
    "        torch.save(upsampled_batch, output_batch_path)\n",
    "\n",
    "    print(\"Upsampled data saved.\")\n",
    "    \n",
    "def extract_center_voxel_channels(batch_dir='upsampled_batches'):\n",
    "    \n",
    "    center_voxel_channels = []\n",
    "\n",
    "    # Get list of all .pt files in the batch directory\n",
    "    batch_files = [f for f in os.listdir(batch_dir) if f.endswith('.pt')]\n",
    "    batch_files.sort()\n",
    "\n",
    "    for batch_file in tqdm(batch_files, desc='Processing batches'):\n",
    "        batch_path = os.path.join(batch_dir, batch_file)\n",
    "        # Load the batch data\n",
    "        batch_data = torch.load(batch_path)  # Should be a list of (tensor, point_idx)\n",
    "\n",
    "        for data in batch_data:\n",
    "            upsampled_tensor, point_idx = data  # Assuming data is (tensor, point_idx)\n",
    "            # upsampled_tensor shape: [D, H, W, C], expected to be [16, 16, 16, 3]\n",
    "\n",
    "            # Verify tensor shape\n",
    "            if upsampled_tensor.shape != (16, 16, 16, 3):\n",
    "                raise ValueError(f\"Unexpected tensor shape: {upsampled_tensor.shape}\")\n",
    "\n",
    "            # Get the center voxel indices\n",
    "            center_idx = (7, 7, 7)  # Zero-based indexing\n",
    "\n",
    "            # Extract the channels at the center voxel\n",
    "            center_channels = upsampled_tensor[center_idx[0], center_idx[1], center_idx[2], :]  # Shape: [3]\n",
    "\n",
    "            # Append to the list\n",
    "            center_voxel_channels.append((center_channels, point_idx))\n",
    "\n",
    "    return center_voxel_channels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Directory containing the upsampled batches\n",
    "    batch_dir_up = 'upsampled_batches'\n",
    "    batch_dir_ori = 'tensor_batches'\n",
    "\n",
    "    # Extract the center voxel channels\n",
    "    center_voxel_data_up = extract_center_voxel_channels(batch_dir_up)\n",
    "    center_voxel_data_ori = extract_center_voxel_channels(batch_dir_ori)\n",
    "    \n",
    "    channels_list_up = [channels.numpy() for channels, _ in center_voxel_data_up]\n",
    "    channels_list_ori = [channels.numpy() for channels, _ in center_voxel_data_ori]\n",
    "    \n",
    "    channels_array_up = np.array(channels_list_up)\n",
    "    channels_array_ori = np.array(channels_list_ori)\n",
    "\n",
    "    np.savez('center_voxel_data.npz', channels_up=channels_array_up, channels_ori=channels_array_ori)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to consider...\n",
    "\n",
    "- Instead of building a local image for every measurement, we may consider building enough local images from a set of $\\epsilon$-balls that covers the entire space. \n",
    "\n",
    "- We can consider using the engression framework rather than the MSE loss. \n",
    "\n",
    "- When mapping the higher resolution image back to the original space, we can use interpolation to fit a function first, and then evaluate the function at the spatial coordinates to obtain the corresponding values. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
