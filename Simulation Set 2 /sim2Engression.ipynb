{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EngressionSim2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2649,  E(|Y-Yhat|): 0.3780,  E(|Yhat-Yhat'|): 0.2263\n",
      "[Epoch 100 (50%)] energy-loss: 0.2086,  E(|Y-Yhat|): 0.3764,  E(|Yhat-Yhat'|): 0.3357\n",
      "[Epoch 200 (100%)] energy-loss: 0.2090,  E(|Y-Yhat|): 0.3758,  E(|Yhat-Yhat'|): 0.3336\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2093,  E(|Y-Yhat|): 0.3768,  E(|Yhat-Yhat'|): 0.3351\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3016, 0.8584])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3252,  E(|Y-Yhat|): 0.3914,  E(|Yhat-Yhat'|): 0.1324\n",
      "[Epoch 100 (50%)] energy-loss: 0.2107,  E(|Y-Yhat|): 0.3735,  E(|Yhat-Yhat'|): 0.3256\n",
      "[Epoch 200 (100%)] energy-loss: 0.2096,  E(|Y-Yhat|): 0.3714,  E(|Yhat-Yhat'|): 0.3236\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2105,  E(|Y-Yhat|): 0.3735,  E(|Yhat-Yhat'|): 0.3261\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3605, 0.7911])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3958,  E(|Y-Yhat|): 0.4309,  E(|Yhat-Yhat'|): 0.0701\n",
      "[Epoch 100 (50%)] energy-loss: 0.2125,  E(|Y-Yhat|): 0.3820,  E(|Yhat-Yhat'|): 0.3390\n",
      "[Epoch 200 (100%)] energy-loss: 0.2107,  E(|Y-Yhat|): 0.3764,  E(|Yhat-Yhat'|): 0.3314\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2119,  E(|Y-Yhat|): 0.3765,  E(|Yhat-Yhat'|): 0.3292\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.4307, 0.8017])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3911,  E(|Y-Yhat|): 0.4456,  E(|Yhat-Yhat'|): 0.1089\n",
      "[Epoch 100 (50%)] energy-loss: 0.2115,  E(|Y-Yhat|): 0.3802,  E(|Yhat-Yhat'|): 0.3374\n",
      "[Epoch 200 (100%)] energy-loss: 0.2085,  E(|Y-Yhat|): 0.3829,  E(|Yhat-Yhat'|): 0.3488\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2111,  E(|Y-Yhat|): 0.3811,  E(|Yhat-Yhat'|): 0.3399\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.4083, 0.6692])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2723,  E(|Y-Yhat|): 0.4349,  E(|Yhat-Yhat'|): 0.3251\n",
      "[Epoch 100 (50%)] energy-loss: 0.2110,  E(|Y-Yhat|): 0.3755,  E(|Yhat-Yhat'|): 0.3289\n",
      "[Epoch 200 (100%)] energy-loss: 0.2103,  E(|Y-Yhat|): 0.3735,  E(|Yhat-Yhat'|): 0.3264\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2073,  E(|Y-Yhat|): 0.3693,  E(|Yhat-Yhat'|): 0.3240\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3563, 0.6707])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3032,  E(|Y-Yhat|): 0.4427,  E(|Yhat-Yhat'|): 0.2789\n",
      "[Epoch 100 (50%)] energy-loss: 0.2119,  E(|Y-Yhat|): 0.3712,  E(|Yhat-Yhat'|): 0.3187\n",
      "[Epoch 200 (100%)] energy-loss: 0.2090,  E(|Y-Yhat|): 0.3685,  E(|Yhat-Yhat'|): 0.3189\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2085,  E(|Y-Yhat|): 0.3736,  E(|Yhat-Yhat'|): 0.3302\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.2651, 0.6193])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3678,  E(|Y-Yhat|): 0.4692,  E(|Yhat-Yhat'|): 0.2027\n",
      "[Epoch 100 (50%)] energy-loss: 0.2179,  E(|Y-Yhat|): 0.3779,  E(|Yhat-Yhat'|): 0.3200\n",
      "[Epoch 200 (100%)] energy-loss: 0.2183,  E(|Y-Yhat|): 0.3778,  E(|Yhat-Yhat'|): 0.3190\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2176,  E(|Y-Yhat|): 0.3870,  E(|Yhat-Yhat'|): 0.3388\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3774, 0.4728])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2280,  E(|Y-Yhat|): 0.3824,  E(|Yhat-Yhat'|): 0.3088\n",
      "[Epoch 100 (50%)] energy-loss: 0.2082,  E(|Y-Yhat|): 0.3735,  E(|Yhat-Yhat'|): 0.3307\n",
      "[Epoch 200 (100%)] energy-loss: 0.2095,  E(|Y-Yhat|): 0.3761,  E(|Yhat-Yhat'|): 0.3333\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2079,  E(|Y-Yhat|): 0.3749,  E(|Yhat-Yhat'|): 0.3340\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.2484, 0.4509])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2927,  E(|Y-Yhat|): 0.4206,  E(|Yhat-Yhat'|): 0.2557\n",
      "[Epoch 100 (50%)] energy-loss: 0.2099,  E(|Y-Yhat|): 0.3685,  E(|Yhat-Yhat'|): 0.3172\n",
      "[Epoch 200 (100%)] energy-loss: 0.2092,  E(|Y-Yhat|): 0.3740,  E(|Yhat-Yhat'|): 0.3295\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2116,  E(|Y-Yhat|): 0.3775,  E(|Yhat-Yhat'|): 0.3318\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1796, 0.5036])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4025,  E(|Y-Yhat|): 0.4339,  E(|Yhat-Yhat'|): 0.0629\n",
      "[Epoch 100 (50%)] energy-loss: 0.2052,  E(|Y-Yhat|): 0.3688,  E(|Yhat-Yhat'|): 0.3272\n",
      "[Epoch 200 (100%)] energy-loss: 0.2101,  E(|Y-Yhat|): 0.3726,  E(|Yhat-Yhat'|): 0.3251\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2095,  E(|Y-Yhat|): 0.3739,  E(|Yhat-Yhat'|): 0.3290\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1853, 0.6280])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4279,  E(|Y-Yhat|): 0.4581,  E(|Yhat-Yhat'|): 0.0604\n",
      "[Epoch 100 (50%)] energy-loss: 0.2075,  E(|Y-Yhat|): 0.3766,  E(|Yhat-Yhat'|): 0.3382\n",
      "[Epoch 200 (100%)] energy-loss: 0.2036,  E(|Y-Yhat|): 0.3714,  E(|Yhat-Yhat'|): 0.3357\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2054,  E(|Y-Yhat|): 0.3722,  E(|Yhat-Yhat'|): 0.3334\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3087, 0.5831])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4087,  E(|Y-Yhat|): 0.4614,  E(|Yhat-Yhat'|): 0.1055\n",
      "[Epoch 100 (50%)] energy-loss: 0.2052,  E(|Y-Yhat|): 0.3843,  E(|Yhat-Yhat'|): 0.3581\n",
      "[Epoch 200 (100%)] energy-loss: 0.2079,  E(|Y-Yhat|): 0.3873,  E(|Yhat-Yhat'|): 0.3587\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2122,  E(|Y-Yhat|): 0.3892,  E(|Yhat-Yhat'|): 0.3541\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3142, 0.5828])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2843,  E(|Y-Yhat|): 0.4279,  E(|Yhat-Yhat'|): 0.2872\n",
      "[Epoch 100 (50%)] energy-loss: 0.1984,  E(|Y-Yhat|): 0.3914,  E(|Yhat-Yhat'|): 0.3859\n",
      "[Epoch 200 (100%)] energy-loss: 0.2028,  E(|Y-Yhat|): 0.3916,  E(|Yhat-Yhat'|): 0.3776\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2051,  E(|Y-Yhat|): 0.3943,  E(|Yhat-Yhat'|): 0.3785\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3829, 0.5622])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4729,  E(|Y-Yhat|): 0.5244,  E(|Yhat-Yhat'|): 0.1029\n",
      "[Epoch 100 (50%)] energy-loss: 0.2041,  E(|Y-Yhat|): 0.3907,  E(|Yhat-Yhat'|): 0.3732\n",
      "[Epoch 200 (100%)] energy-loss: 0.2044,  E(|Y-Yhat|): 0.3880,  E(|Yhat-Yhat'|): 0.3673\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2061,  E(|Y-Yhat|): 0.3903,  E(|Yhat-Yhat'|): 0.3686\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3956, 0.5921])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4273,  E(|Y-Yhat|): 0.4985,  E(|Yhat-Yhat'|): 0.1425\n",
      "[Epoch 100 (50%)] energy-loss: 0.1976,  E(|Y-Yhat|): 0.3785,  E(|Yhat-Yhat'|): 0.3617\n",
      "[Epoch 200 (100%)] energy-loss: 0.2024,  E(|Y-Yhat|): 0.3825,  E(|Yhat-Yhat'|): 0.3601\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1992,  E(|Y-Yhat|): 0.3834,  E(|Yhat-Yhat'|): 0.3684\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.4806, 0.5927])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2675,  E(|Y-Yhat|): 0.4247,  E(|Yhat-Yhat'|): 0.3145\n",
      "[Epoch 100 (50%)] energy-loss: 0.1998,  E(|Y-Yhat|): 0.3818,  E(|Yhat-Yhat'|): 0.3640\n",
      "[Epoch 200 (100%)] energy-loss: 0.2014,  E(|Y-Yhat|): 0.3821,  E(|Yhat-Yhat'|): 0.3615\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2026,  E(|Y-Yhat|): 0.3844,  E(|Yhat-Yhat'|): 0.3637\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3248, 0.5776])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4029,  E(|Y-Yhat|): 0.4813,  E(|Yhat-Yhat'|): 0.1568\n",
      "[Epoch 100 (50%)] energy-loss: 0.2075,  E(|Y-Yhat|): 0.3926,  E(|Yhat-Yhat'|): 0.3702\n",
      "[Epoch 200 (100%)] energy-loss: 0.2063,  E(|Y-Yhat|): 0.3918,  E(|Yhat-Yhat'|): 0.3711\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2017,  E(|Y-Yhat|): 0.3885,  E(|Yhat-Yhat'|): 0.3735\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3001, 0.6335])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3329,  E(|Y-Yhat|): 0.4395,  E(|Yhat-Yhat'|): 0.2132\n",
      "[Epoch 100 (50%)] energy-loss: 0.2100,  E(|Y-Yhat|): 0.3842,  E(|Yhat-Yhat'|): 0.3484\n",
      "[Epoch 200 (100%)] energy-loss: 0.2111,  E(|Y-Yhat|): 0.3788,  E(|Yhat-Yhat'|): 0.3354\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2071,  E(|Y-Yhat|): 0.3729,  E(|Yhat-Yhat'|): 0.3316\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.2141, 0.5092])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3672,  E(|Y-Yhat|): 0.4225,  E(|Yhat-Yhat'|): 0.1106\n",
      "[Epoch 100 (50%)] energy-loss: 0.2084,  E(|Y-Yhat|): 0.3772,  E(|Yhat-Yhat'|): 0.3377\n",
      "[Epoch 200 (100%)] energy-loss: 0.2114,  E(|Y-Yhat|): 0.3734,  E(|Yhat-Yhat'|): 0.3239\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2129,  E(|Y-Yhat|): 0.3775,  E(|Yhat-Yhat'|): 0.3294\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.2361, 0.5305])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4155,  E(|Y-Yhat|): 0.4485,  E(|Yhat-Yhat'|): 0.0660\n",
      "[Epoch 100 (50%)] energy-loss: 0.2124,  E(|Y-Yhat|): 0.3789,  E(|Yhat-Yhat'|): 0.3331\n",
      "[Epoch 200 (100%)] energy-loss: 0.2125,  E(|Y-Yhat|): 0.3767,  E(|Yhat-Yhat'|): 0.3283\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2102,  E(|Y-Yhat|): 0.3756,  E(|Yhat-Yhat'|): 0.3308\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0981, 0.5401])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"log\"\n",
    "noise_dist = \"gaussian\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results1 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2982,  E(|Y-Yhat|): 0.4113,  E(|Yhat-Yhat'|): 0.2262\n",
      "[Epoch 100 (50%)] energy-loss: 0.2699,  E(|Y-Yhat|): 0.4500,  E(|Yhat-Yhat'|): 0.3604\n",
      "[Epoch 200 (100%)] energy-loss: 0.2652,  E(|Y-Yhat|): 0.4498,  E(|Yhat-Yhat'|): 0.3691\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2685,  E(|Y-Yhat|): 0.4419,  E(|Yhat-Yhat'|): 0.3469\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0601, 0.5533])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3431,  E(|Y-Yhat|): 0.4091,  E(|Yhat-Yhat'|): 0.1321\n",
      "[Epoch 100 (50%)] energy-loss: 0.2699,  E(|Y-Yhat|): 0.4444,  E(|Yhat-Yhat'|): 0.3490\n",
      "[Epoch 200 (100%)] energy-loss: 0.2747,  E(|Y-Yhat|): 0.4431,  E(|Yhat-Yhat'|): 0.3369\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2735,  E(|Y-Yhat|): 0.4362,  E(|Yhat-Yhat'|): 0.3254\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1391, 0.4715])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3936,  E(|Y-Yhat|): 0.4287,  E(|Yhat-Yhat'|): 0.0702\n",
      "[Epoch 100 (50%)] energy-loss: 0.2759,  E(|Y-Yhat|): 0.4505,  E(|Yhat-Yhat'|): 0.3492\n",
      "[Epoch 200 (100%)] energy-loss: 0.2762,  E(|Y-Yhat|): 0.4488,  E(|Yhat-Yhat'|): 0.3452\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2756,  E(|Y-Yhat|): 0.4461,  E(|Yhat-Yhat'|): 0.3412\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.2237, 0.5338])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3923,  E(|Y-Yhat|): 0.4467,  E(|Yhat-Yhat'|): 0.1089\n",
      "[Epoch 100 (50%)] energy-loss: 0.2771,  E(|Y-Yhat|): 0.4531,  E(|Yhat-Yhat'|): 0.3520\n",
      "[Epoch 200 (100%)] energy-loss: 0.2716,  E(|Y-Yhat|): 0.4467,  E(|Yhat-Yhat'|): 0.3502\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2765,  E(|Y-Yhat|): 0.4441,  E(|Yhat-Yhat'|): 0.3351\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1824, 0.3931])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3282,  E(|Y-Yhat|): 0.4903,  E(|Yhat-Yhat'|): 0.3242\n",
      "[Epoch 100 (50%)] energy-loss: 0.2737,  E(|Y-Yhat|): 0.4490,  E(|Yhat-Yhat'|): 0.3507\n",
      "[Epoch 200 (100%)] energy-loss: 0.2724,  E(|Y-Yhat|): 0.4439,  E(|Yhat-Yhat'|): 0.3430\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2728,  E(|Y-Yhat|): 0.4407,  E(|Yhat-Yhat'|): 0.3358\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1011, 0.3765])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3517,  E(|Y-Yhat|): 0.4903,  E(|Yhat-Yhat'|): 0.2773\n",
      "[Epoch 100 (50%)] energy-loss: 0.2731,  E(|Y-Yhat|): 0.4461,  E(|Yhat-Yhat'|): 0.3460\n",
      "[Epoch 200 (100%)] energy-loss: 0.2772,  E(|Y-Yhat|): 0.4412,  E(|Yhat-Yhat'|): 0.3279\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2718,  E(|Y-Yhat|): 0.4351,  E(|Yhat-Yhat'|): 0.3266\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.9586, 0.2993])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4008,  E(|Y-Yhat|): 0.5021,  E(|Yhat-Yhat'|): 0.2026\n",
      "[Epoch 100 (50%)] energy-loss: 0.2765,  E(|Y-Yhat|): 0.4396,  E(|Yhat-Yhat'|): 0.3264\n",
      "[Epoch 200 (100%)] energy-loss: 0.2770,  E(|Y-Yhat|): 0.4357,  E(|Yhat-Yhat'|): 0.3173\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2744,  E(|Y-Yhat|): 0.4328,  E(|Yhat-Yhat'|): 0.3167\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1386, 0.1635])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2800,  E(|Y-Yhat|): 0.4342,  E(|Yhat-Yhat'|): 0.3084\n",
      "[Epoch 100 (50%)] energy-loss: 0.2632,  E(|Y-Yhat|): 0.4371,  E(|Yhat-Yhat'|): 0.3478\n",
      "[Epoch 200 (100%)] energy-loss: 0.2673,  E(|Y-Yhat|): 0.4394,  E(|Yhat-Yhat'|): 0.3441\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2664,  E(|Y-Yhat|): 0.4415,  E(|Yhat-Yhat'|): 0.3501\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0269, 0.0487])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3248,  E(|Y-Yhat|): 0.4522,  E(|Yhat-Yhat'|): 0.2548\n",
      "[Epoch 100 (50%)] energy-loss: 0.2675,  E(|Y-Yhat|): 0.4360,  E(|Yhat-Yhat'|): 0.3370\n",
      "[Epoch 200 (100%)] energy-loss: 0.2642,  E(|Y-Yhat|): 0.4307,  E(|Yhat-Yhat'|): 0.3330\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2703,  E(|Y-Yhat|): 0.4291,  E(|Yhat-Yhat'|): 0.3177\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.9603, 0.1116])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4014,  E(|Y-Yhat|): 0.4327,  E(|Yhat-Yhat'|): 0.0626\n",
      "[Epoch 100 (50%)] energy-loss: 0.2502,  E(|Y-Yhat|): 0.4320,  E(|Yhat-Yhat'|): 0.3636\n",
      "[Epoch 200 (100%)] energy-loss: 0.2514,  E(|Y-Yhat|): 0.4264,  E(|Yhat-Yhat'|): 0.3500\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2526,  E(|Y-Yhat|): 0.4295,  E(|Yhat-Yhat'|): 0.3538\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.9021, 0.2811])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4150,  E(|Y-Yhat|): 0.4451,  E(|Yhat-Yhat'|): 0.0602\n",
      "[Epoch 100 (50%)] energy-loss: 0.2524,  E(|Y-Yhat|): 0.4345,  E(|Yhat-Yhat'|): 0.3641\n",
      "[Epoch 200 (100%)] energy-loss: 0.2542,  E(|Y-Yhat|): 0.4339,  E(|Yhat-Yhat'|): 0.3594\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2482,  E(|Y-Yhat|): 0.4304,  E(|Yhat-Yhat'|): 0.3643\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0572, 0.1443])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4000,  E(|Y-Yhat|): 0.4529,  E(|Yhat-Yhat'|): 0.1058\n",
      "[Epoch 100 (50%)] energy-loss: 0.2563,  E(|Y-Yhat|): 0.4449,  E(|Yhat-Yhat'|): 0.3772\n",
      "[Epoch 200 (100%)] energy-loss: 0.2562,  E(|Y-Yhat|): 0.4449,  E(|Yhat-Yhat'|): 0.3774\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2587,  E(|Y-Yhat|): 0.4444,  E(|Yhat-Yhat'|): 0.3714\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0666, 0.1773])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2837,  E(|Y-Yhat|): 0.4275,  E(|Yhat-Yhat'|): 0.2874\n",
      "[Epoch 100 (50%)] energy-loss: 0.2470,  E(|Y-Yhat|): 0.4547,  E(|Yhat-Yhat'|): 0.4154\n",
      "[Epoch 200 (100%)] energy-loss: 0.2467,  E(|Y-Yhat|): 0.4526,  E(|Yhat-Yhat'|): 0.4117\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2486,  E(|Y-Yhat|): 0.4519,  E(|Yhat-Yhat'|): 0.4067\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1684, 0.1959])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4424,  E(|Y-Yhat|): 0.4937,  E(|Yhat-Yhat'|): 0.1025\n",
      "[Epoch 100 (50%)] energy-loss: 0.2555,  E(|Y-Yhat|): 0.4510,  E(|Yhat-Yhat'|): 0.3910\n",
      "[Epoch 200 (100%)] energy-loss: 0.2508,  E(|Y-Yhat|): 0.4487,  E(|Yhat-Yhat'|): 0.3956\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2554,  E(|Y-Yhat|): 0.4499,  E(|Yhat-Yhat'|): 0.3890\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.2168, 0.2265])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3997,  E(|Y-Yhat|): 0.4713,  E(|Yhat-Yhat'|): 0.1431\n",
      "[Epoch 100 (50%)] energy-loss: 0.2520,  E(|Y-Yhat|): 0.4489,  E(|Yhat-Yhat'|): 0.3937\n",
      "[Epoch 200 (100%)] energy-loss: 0.2500,  E(|Y-Yhat|): 0.4459,  E(|Yhat-Yhat'|): 0.3918\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2517,  E(|Y-Yhat|): 0.4553,  E(|Yhat-Yhat'|): 0.4072\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.3586, 0.2395])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2873,  E(|Y-Yhat|): 0.4445,  E(|Yhat-Yhat'|): 0.3144\n",
      "[Epoch 100 (50%)] energy-loss: 0.2551,  E(|Y-Yhat|): 0.4561,  E(|Yhat-Yhat'|): 0.4019\n",
      "[Epoch 200 (100%)] energy-loss: 0.2526,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4047\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2536,  E(|Y-Yhat|): 0.4615,  E(|Yhat-Yhat'|): 0.4157\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1823, 0.2037])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3940,  E(|Y-Yhat|): 0.4725,  E(|Yhat-Yhat'|): 0.1570\n",
      "[Epoch 100 (50%)] energy-loss: 0.2516,  E(|Y-Yhat|): 0.4544,  E(|Yhat-Yhat'|): 0.4056\n",
      "[Epoch 200 (100%)] energy-loss: 0.2544,  E(|Y-Yhat|): 0.4533,  E(|Yhat-Yhat'|): 0.3979\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2522,  E(|Y-Yhat|): 0.4540,  E(|Yhat-Yhat'|): 0.4037\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.1219, 0.2491])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3564,  E(|Y-Yhat|): 0.4631,  E(|Yhat-Yhat'|): 0.2133\n",
      "[Epoch 100 (50%)] energy-loss: 0.2555,  E(|Y-Yhat|): 0.4456,  E(|Yhat-Yhat'|): 0.3803\n",
      "[Epoch 200 (100%)] energy-loss: 0.2540,  E(|Y-Yhat|): 0.4506,  E(|Yhat-Yhat'|): 0.3933\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2576,  E(|Y-Yhat|): 0.4570,  E(|Yhat-Yhat'|): 0.3988\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0231, 0.0873])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3770,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.1107\n",
      "[Epoch 100 (50%)] energy-loss: 0.2558,  E(|Y-Yhat|): 0.4338,  E(|Yhat-Yhat'|): 0.3561\n",
      "[Epoch 200 (100%)] energy-loss: 0.2566,  E(|Y-Yhat|): 0.4366,  E(|Yhat-Yhat'|): 0.3600\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2572,  E(|Y-Yhat|): 0.4419,  E(|Yhat-Yhat'|): 0.3694\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 1.0126, 0.1375])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4121,  E(|Y-Yhat|): 0.4449,  E(|Yhat-Yhat'|): 0.0657\n",
      "[Epoch 100 (50%)] energy-loss: 0.2591,  E(|Y-Yhat|): 0.4427,  E(|Yhat-Yhat'|): 0.3673\n",
      "[Epoch 200 (100%)] energy-loss: 0.2611,  E(|Y-Yhat|): 0.4413,  E(|Yhat-Yhat'|): 0.3604\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2592,  E(|Y-Yhat|): 0.4441,  E(|Yhat-Yhat'|): 0.3698\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.8695, 0.1248])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"log\"\n",
    "noise_dist = \"uniform\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "results2 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3169,  E(|Y-Yhat|): 0.4301,  E(|Yhat-Yhat'|): 0.2265\n",
      "[Epoch 100 (50%)] energy-loss: 0.2167,  E(|Y-Yhat|): 0.4385,  E(|Yhat-Yhat'|): 0.4435\n",
      "[Epoch 200 (100%)] energy-loss: 0.2140,  E(|Y-Yhat|): 0.4302,  E(|Yhat-Yhat'|): 0.4324\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2132,  E(|Y-Yhat|): 0.4351,  E(|Yhat-Yhat'|): 0.4437\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.8231, 0.0987])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3834,  E(|Y-Yhat|): 0.4492,  E(|Yhat-Yhat'|): 0.1317\n",
      "[Epoch 100 (50%)] energy-loss: 0.2044,  E(|Y-Yhat|): 0.4162,  E(|Yhat-Yhat'|): 0.4237\n",
      "[Epoch 200 (100%)] energy-loss: 0.2056,  E(|Y-Yhat|): 0.4171,  E(|Yhat-Yhat'|): 0.4230\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2053,  E(|Y-Yhat|): 0.4159,  E(|Yhat-Yhat'|): 0.4212\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.8792, 0.0706])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4301,  E(|Y-Yhat|): 0.4653,  E(|Yhat-Yhat'|): 0.0703\n",
      "[Epoch 100 (50%)] energy-loss: 0.2026,  E(|Y-Yhat|): 0.4090,  E(|Yhat-Yhat'|): 0.4128\n",
      "[Epoch 200 (100%)] energy-loss: 0.2050,  E(|Y-Yhat|): 0.4026,  E(|Yhat-Yhat'|): 0.3952\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2006,  E(|Y-Yhat|): 0.4031,  E(|Yhat-Yhat'|): 0.4051\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.9129, 0.1073])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4218,  E(|Y-Yhat|): 0.4764,  E(|Yhat-Yhat'|): 0.1092\n",
      "[Epoch 100 (50%)] energy-loss: 0.2059,  E(|Y-Yhat|): 0.4209,  E(|Yhat-Yhat'|): 0.4300\n",
      "[Epoch 200 (100%)] energy-loss: 0.2035,  E(|Y-Yhat|): 0.4088,  E(|Yhat-Yhat'|): 0.4106\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2055,  E(|Y-Yhat|): 0.4126,  E(|Yhat-Yhat'|): 0.4143\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8340, -0.0170])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2656,  E(|Y-Yhat|): 0.4280,  E(|Yhat-Yhat'|): 0.3248\n",
      "[Epoch 100 (50%)] energy-loss: 0.1968,  E(|Y-Yhat|): 0.3897,  E(|Yhat-Yhat'|): 0.3859\n",
      "[Epoch 200 (100%)] energy-loss: 0.1961,  E(|Y-Yhat|): 0.3904,  E(|Yhat-Yhat'|): 0.3886\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1956,  E(|Y-Yhat|): 0.3881,  E(|Yhat-Yhat'|): 0.3849\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7998, -0.0142])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2938,  E(|Y-Yhat|): 0.4320,  E(|Yhat-Yhat'|): 0.2763\n",
      "[Epoch 100 (50%)] energy-loss: 0.1894,  E(|Y-Yhat|): 0.3886,  E(|Yhat-Yhat'|): 0.3984\n",
      "[Epoch 200 (100%)] energy-loss: 0.1882,  E(|Y-Yhat|): 0.3830,  E(|Yhat-Yhat'|): 0.3895\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1866,  E(|Y-Yhat|): 0.3805,  E(|Yhat-Yhat'|): 0.3879\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7361, -0.1261])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3855,  E(|Y-Yhat|): 0.4866,  E(|Yhat-Yhat'|): 0.2023\n",
      "[Epoch 100 (50%)] energy-loss: 0.2034,  E(|Y-Yhat|): 0.4031,  E(|Yhat-Yhat'|): 0.3995\n",
      "[Epoch 200 (100%)] energy-loss: 0.2057,  E(|Y-Yhat|): 0.4045,  E(|Yhat-Yhat'|): 0.3975\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2028,  E(|Y-Yhat|): 0.4073,  E(|Yhat-Yhat'|): 0.4089\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8372, -0.1171])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2593,  E(|Y-Yhat|): 0.4134,  E(|Yhat-Yhat'|): 0.3082\n",
      "[Epoch 100 (50%)] energy-loss: 0.2136,  E(|Y-Yhat|): 0.4344,  E(|Yhat-Yhat'|): 0.4415\n",
      "[Epoch 200 (100%)] energy-loss: 0.2209,  E(|Y-Yhat|): 0.4373,  E(|Yhat-Yhat'|): 0.4329\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2165,  E(|Y-Yhat|): 0.4329,  E(|Yhat-Yhat'|): 0.4329\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8297, -0.2098])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3560,  E(|Y-Yhat|): 0.4835,  E(|Yhat-Yhat'|): 0.2548\n",
      "[Epoch 100 (50%)] energy-loss: 0.2172,  E(|Y-Yhat|): 0.4362,  E(|Yhat-Yhat'|): 0.4381\n",
      "[Epoch 200 (100%)] energy-loss: 0.2149,  E(|Y-Yhat|): 0.4338,  E(|Yhat-Yhat'|): 0.4379\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2221,  E(|Y-Yhat|): 0.4411,  E(|Yhat-Yhat'|): 0.4380\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8115, -0.1574])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5295,  E(|Y-Yhat|): 0.5608,  E(|Yhat-Yhat'|): 0.0626\n",
      "[Epoch 100 (50%)] energy-loss: 0.2204,  E(|Y-Yhat|): 0.4457,  E(|Yhat-Yhat'|): 0.4507\n",
      "[Epoch 200 (100%)] energy-loss: 0.2251,  E(|Y-Yhat|): 0.4500,  E(|Yhat-Yhat'|): 0.4499\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2242,  E(|Y-Yhat|): 0.4560,  E(|Yhat-Yhat'|): 0.4635\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7899, -0.0328])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5456,  E(|Y-Yhat|): 0.5757,  E(|Yhat-Yhat'|): 0.0602\n",
      "[Epoch 100 (50%)] energy-loss: 0.2290,  E(|Y-Yhat|): 0.4571,  E(|Yhat-Yhat'|): 0.4562\n",
      "[Epoch 200 (100%)] energy-loss: 0.2268,  E(|Y-Yhat|): 0.4666,  E(|Yhat-Yhat'|): 0.4795\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2325,  E(|Y-Yhat|): 0.4681,  E(|Yhat-Yhat'|): 0.4712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9232, -0.1573])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4936,  E(|Y-Yhat|): 0.5468,  E(|Yhat-Yhat'|): 0.1064\n",
      "[Epoch 100 (50%)] energy-loss: 0.2307,  E(|Y-Yhat|): 0.4668,  E(|Yhat-Yhat'|): 0.4722\n",
      "[Epoch 200 (100%)] energy-loss: 0.2306,  E(|Y-Yhat|): 0.4679,  E(|Yhat-Yhat'|): 0.4746\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2286,  E(|Y-Yhat|): 0.4695,  E(|Yhat-Yhat'|): 0.4819\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8962, -0.1530])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3372,  E(|Y-Yhat|): 0.4811,  E(|Yhat-Yhat'|): 0.2876\n",
      "[Epoch 100 (50%)] energy-loss: 0.2480,  E(|Y-Yhat|): 0.4967,  E(|Yhat-Yhat'|): 0.4973\n",
      "[Epoch 200 (100%)] energy-loss: 0.2504,  E(|Y-Yhat|): 0.4956,  E(|Yhat-Yhat'|): 0.4904\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2447,  E(|Y-Yhat|): 0.4962,  E(|Yhat-Yhat'|): 0.5031\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9890, -0.1423])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5218,  E(|Y-Yhat|): 0.5731,  E(|Yhat-Yhat'|): 0.1025\n",
      "[Epoch 100 (50%)] energy-loss: 0.2319,  E(|Y-Yhat|): 0.4707,  E(|Yhat-Yhat'|): 0.4777\n",
      "[Epoch 200 (100%)] energy-loss: 0.2345,  E(|Y-Yhat|): 0.4646,  E(|Yhat-Yhat'|): 0.4603\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2333,  E(|Y-Yhat|): 0.4661,  E(|Yhat-Yhat'|): 0.4655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0561, -0.1120])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4942,  E(|Y-Yhat|): 0.5659,  E(|Yhat-Yhat'|): 0.1433\n",
      "[Epoch 100 (50%)] energy-loss: 0.2360,  E(|Y-Yhat|): 0.4735,  E(|Yhat-Yhat'|): 0.4749\n",
      "[Epoch 200 (100%)] energy-loss: 0.2369,  E(|Y-Yhat|): 0.4806,  E(|Yhat-Yhat'|): 0.4874\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2382,  E(|Y-Yhat|): 0.4868,  E(|Yhat-Yhat'|): 0.4972\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.1742, -0.0672])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3545,  E(|Y-Yhat|): 0.5112,  E(|Yhat-Yhat'|): 0.3134\n",
      "[Epoch 100 (50%)] energy-loss: 0.2513,  E(|Y-Yhat|): 0.4936,  E(|Yhat-Yhat'|): 0.4847\n",
      "[Epoch 200 (100%)] energy-loss: 0.2492,  E(|Y-Yhat|): 0.5173,  E(|Yhat-Yhat'|): 0.5362\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2537,  E(|Y-Yhat|): 0.5266,  E(|Yhat-Yhat'|): 0.5459\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0663, -0.0513])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5879,  E(|Y-Yhat|): 0.6663,  E(|Yhat-Yhat'|): 0.1568\n",
      "[Epoch 100 (50%)] energy-loss: 0.2730,  E(|Y-Yhat|): 0.5468,  E(|Yhat-Yhat'|): 0.5476\n",
      "[Epoch 200 (100%)] energy-loss: 0.2737,  E(|Y-Yhat|): 0.5475,  E(|Yhat-Yhat'|): 0.5475\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2735,  E(|Y-Yhat|): 0.5528,  E(|Yhat-Yhat'|): 0.5587\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0379, -0.0252])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5218,  E(|Y-Yhat|): 0.6280,  E(|Yhat-Yhat'|): 0.2125\n",
      "[Epoch 100 (50%)] energy-loss: 0.2654,  E(|Y-Yhat|): 0.5327,  E(|Yhat-Yhat'|): 0.5346\n",
      "[Epoch 200 (100%)] energy-loss: 0.2688,  E(|Y-Yhat|): 0.5451,  E(|Yhat-Yhat'|): 0.5526\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2665,  E(|Y-Yhat|): 0.5521,  E(|Yhat-Yhat'|): 0.5711\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0321, -0.1672])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5534,  E(|Y-Yhat|): 0.6082,  E(|Yhat-Yhat'|): 0.1098\n",
      "[Epoch 100 (50%)] energy-loss: 0.2419,  E(|Y-Yhat|): 0.4899,  E(|Yhat-Yhat'|): 0.4960\n",
      "[Epoch 200 (100%)] energy-loss: 0.2467,  E(|Y-Yhat|): 0.4938,  E(|Yhat-Yhat'|): 0.4943\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2449,  E(|Y-Yhat|): 0.4950,  E(|Yhat-Yhat'|): 0.5001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9503, -0.1064])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5877,  E(|Y-Yhat|): 0.6206,  E(|Yhat-Yhat'|): 0.0659\n",
      "[Epoch 100 (50%)] energy-loss: 0.2426,  E(|Y-Yhat|): 0.4948,  E(|Yhat-Yhat'|): 0.5044\n",
      "[Epoch 200 (100%)] energy-loss: 0.2465,  E(|Y-Yhat|): 0.4928,  E(|Yhat-Yhat'|): 0.4924\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2475,  E(|Y-Yhat|): 0.4991,  E(|Yhat-Yhat'|): 0.5031\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8760, -0.0932])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4008,  E(|Y-Yhat|): 0.5136,  E(|Yhat-Yhat'|): 0.2257\n",
      "[Epoch 100 (50%)] energy-loss: 0.2715,  E(|Y-Yhat|): 0.5460,  E(|Yhat-Yhat'|): 0.5489\n",
      "[Epoch 200 (100%)] energy-loss: 0.2725,  E(|Y-Yhat|): 0.5478,  E(|Yhat-Yhat'|): 0.5507\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2742,  E(|Y-Yhat|): 0.5534,  E(|Yhat-Yhat'|): 0.5585\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8155, -0.0880])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4607,  E(|Y-Yhat|): 0.5262,  E(|Yhat-Yhat'|): 0.1310\n",
      "[Epoch 100 (50%)] energy-loss: 0.2587,  E(|Y-Yhat|): 0.5268,  E(|Yhat-Yhat'|): 0.5363\n",
      "[Epoch 200 (100%)] energy-loss: 0.2616,  E(|Y-Yhat|): 0.5250,  E(|Yhat-Yhat'|): 0.5268\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2560,  E(|Y-Yhat|): 0.5239,  E(|Yhat-Yhat'|): 0.5359\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8868, -0.0930])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4929,  E(|Y-Yhat|): 0.5278,  E(|Yhat-Yhat'|): 0.0699\n",
      "[Epoch 100 (50%)] energy-loss: 0.2510,  E(|Y-Yhat|): 0.5106,  E(|Yhat-Yhat'|): 0.5191\n",
      "[Epoch 200 (100%)] energy-loss: 0.2509,  E(|Y-Yhat|): 0.5014,  E(|Yhat-Yhat'|): 0.5010\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2538,  E(|Y-Yhat|): 0.5012,  E(|Yhat-Yhat'|): 0.4948\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9312, -0.0467])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5138,  E(|Y-Yhat|): 0.5683,  E(|Yhat-Yhat'|): 0.1090\n",
      "[Epoch 100 (50%)] energy-loss: 0.2671,  E(|Y-Yhat|): 0.5376,  E(|Yhat-Yhat'|): 0.5410\n",
      "[Epoch 200 (100%)] energy-loss: 0.2619,  E(|Y-Yhat|): 0.5262,  E(|Yhat-Yhat'|): 0.5286\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2604,  E(|Y-Yhat|): 0.5282,  E(|Yhat-Yhat'|): 0.5356\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8550, -0.1245])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3515,  E(|Y-Yhat|): 0.5138,  E(|Yhat-Yhat'|): 0.3246\n",
      "[Epoch 100 (50%)] energy-loss: 0.2591,  E(|Y-Yhat|): 0.5203,  E(|Yhat-Yhat'|): 0.5225\n",
      "[Epoch 200 (100%)] energy-loss: 0.2645,  E(|Y-Yhat|): 0.5234,  E(|Yhat-Yhat'|): 0.5180\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2529,  E(|Y-Yhat|): 0.5164,  E(|Yhat-Yhat'|): 0.5270\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8041, -0.1063])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3762,  E(|Y-Yhat|): 0.5139,  E(|Yhat-Yhat'|): 0.2754\n",
      "[Epoch 100 (50%)] energy-loss: 0.2481,  E(|Y-Yhat|): 0.5070,  E(|Yhat-Yhat'|): 0.5177\n",
      "[Epoch 200 (100%)] energy-loss: 0.2498,  E(|Y-Yhat|): 0.5053,  E(|Yhat-Yhat'|): 0.5110\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2500,  E(|Y-Yhat|): 0.5123,  E(|Yhat-Yhat'|): 0.5247\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7006, -0.2337])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4449,  E(|Y-Yhat|): 0.5456,  E(|Yhat-Yhat'|): 0.2013\n",
      "[Epoch 100 (50%)] energy-loss: 0.2477,  E(|Y-Yhat|): 0.5033,  E(|Yhat-Yhat'|): 0.5111\n",
      "[Epoch 200 (100%)] energy-loss: 0.2460,  E(|Y-Yhat|): 0.4943,  E(|Yhat-Yhat'|): 0.4966\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2432,  E(|Y-Yhat|): 0.4955,  E(|Yhat-Yhat'|): 0.5046\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8072, -0.2139])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3281,  E(|Y-Yhat|): 0.4820,  E(|Yhat-Yhat'|): 0.3078\n",
      "[Epoch 100 (50%)] energy-loss: 0.2677,  E(|Y-Yhat|): 0.5276,  E(|Yhat-Yhat'|): 0.5198\n",
      "[Epoch 200 (100%)] energy-loss: 0.2683,  E(|Y-Yhat|): 0.5316,  E(|Yhat-Yhat'|): 0.5266\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2682,  E(|Y-Yhat|): 0.5340,  E(|Yhat-Yhat'|): 0.5317\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7536, -0.3247])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4141,  E(|Y-Yhat|): 0.5411,  E(|Yhat-Yhat'|): 0.2540\n",
      "[Epoch 100 (50%)] energy-loss: 0.2599,  E(|Y-Yhat|): 0.5322,  E(|Yhat-Yhat'|): 0.5446\n",
      "[Epoch 200 (100%)] energy-loss: 0.2689,  E(|Y-Yhat|): 0.5270,  E(|Yhat-Yhat'|): 0.5162\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2664,  E(|Y-Yhat|): 0.5227,  E(|Yhat-Yhat'|): 0.5127\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7435, -0.2716])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6238,  E(|Y-Yhat|): 0.6550,  E(|Yhat-Yhat'|): 0.0624\n",
      "[Epoch 100 (50%)] energy-loss: 0.2792,  E(|Y-Yhat|): 0.5661,  E(|Yhat-Yhat'|): 0.5738\n",
      "[Epoch 200 (100%)] energy-loss: 0.2830,  E(|Y-Yhat|): 0.5747,  E(|Yhat-Yhat'|): 0.5835\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2811,  E(|Y-Yhat|): 0.5688,  E(|Yhat-Yhat'|): 0.5755\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6917, -0.1562])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6873,  E(|Y-Yhat|): 0.7173,  E(|Yhat-Yhat'|): 0.0601\n",
      "[Epoch 100 (50%)] energy-loss: 0.3023,  E(|Y-Yhat|): 0.6164,  E(|Yhat-Yhat'|): 0.6282\n",
      "[Epoch 200 (100%)] energy-loss: 0.3003,  E(|Y-Yhat|): 0.6089,  E(|Yhat-Yhat'|): 0.6173\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2985,  E(|Y-Yhat|): 0.6063,  E(|Yhat-Yhat'|): 0.6158\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8472, -0.3131])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6453,  E(|Y-Yhat|): 0.6984,  E(|Yhat-Yhat'|): 0.1062\n",
      "[Epoch 100 (50%)] energy-loss: 0.3066,  E(|Y-Yhat|): 0.6192,  E(|Yhat-Yhat'|): 0.6252\n",
      "[Epoch 200 (100%)] energy-loss: 0.3096,  E(|Y-Yhat|): 0.6209,  E(|Yhat-Yhat'|): 0.6226\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3153,  E(|Y-Yhat|): 0.6351,  E(|Yhat-Yhat'|): 0.6396\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8378, -0.2848])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.4588,  E(|Y-Yhat|): 0.6024,  E(|Yhat-Yhat'|): 0.2871\n",
      "[Epoch 100 (50%)] energy-loss: 0.3205,  E(|Y-Yhat|): 0.6411,  E(|Yhat-Yhat'|): 0.6412\n",
      "[Epoch 200 (100%)] energy-loss: 0.3299,  E(|Y-Yhat|): 0.6558,  E(|Yhat-Yhat'|): 0.6518\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3193,  E(|Y-Yhat|): 0.6445,  E(|Yhat-Yhat'|): 0.6504\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9255, -0.2666])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6642,  E(|Y-Yhat|): 0.7154,  E(|Yhat-Yhat'|): 0.1024\n",
      "[Epoch 100 (50%)] energy-loss: 0.3058,  E(|Y-Yhat|): 0.6266,  E(|Yhat-Yhat'|): 0.6418\n",
      "[Epoch 200 (100%)] energy-loss: 0.3106,  E(|Y-Yhat|): 0.6261,  E(|Yhat-Yhat'|): 0.6310\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3122,  E(|Y-Yhat|): 0.6387,  E(|Yhat-Yhat'|): 0.6531\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0256, -0.2689])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6551,  E(|Y-Yhat|): 0.7268,  E(|Yhat-Yhat'|): 0.1434\n",
      "[Epoch 100 (50%)] energy-loss: 0.3230,  E(|Y-Yhat|): 0.6523,  E(|Yhat-Yhat'|): 0.6586\n",
      "[Epoch 200 (100%)] energy-loss: 0.3270,  E(|Y-Yhat|): 0.6509,  E(|Yhat-Yhat'|): 0.6478\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3251,  E(|Y-Yhat|): 0.6525,  E(|Yhat-Yhat'|): 0.6548\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.1739, -0.2607])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5001,  E(|Y-Yhat|): 0.6564,  E(|Yhat-Yhat'|): 0.3126\n",
      "[Epoch 100 (50%)] energy-loss: 0.3396,  E(|Y-Yhat|): 0.6905,  E(|Yhat-Yhat'|): 0.7019\n",
      "[Epoch 200 (100%)] energy-loss: 0.3401,  E(|Y-Yhat|): 0.6981,  E(|Yhat-Yhat'|): 0.7160\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3372,  E(|Y-Yhat|): 0.7089,  E(|Yhat-Yhat'|): 0.7433\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0207, -0.2683])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7882,  E(|Y-Yhat|): 0.8667,  E(|Yhat-Yhat'|): 0.1569\n",
      "[Epoch 100 (50%)] energy-loss: 0.3662,  E(|Y-Yhat|): 0.7468,  E(|Yhat-Yhat'|): 0.7613\n",
      "[Epoch 200 (100%)] energy-loss: 0.3693,  E(|Y-Yhat|): 0.7466,  E(|Yhat-Yhat'|): 0.7546\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3751,  E(|Y-Yhat|): 0.7531,  E(|Yhat-Yhat'|): 0.7561\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9326, -0.2428])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7319,  E(|Y-Yhat|): 0.8378,  E(|Yhat-Yhat'|): 0.2118\n",
      "[Epoch 100 (50%)] energy-loss: 0.3662,  E(|Y-Yhat|): 0.7497,  E(|Yhat-Yhat'|): 0.7669\n",
      "[Epoch 200 (100%)] energy-loss: 0.3726,  E(|Y-Yhat|): 0.7374,  E(|Yhat-Yhat'|): 0.7296\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3623,  E(|Y-Yhat|): 0.7349,  E(|Yhat-Yhat'|): 0.7454\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9305, -0.4013])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7202,  E(|Y-Yhat|): 0.7751,  E(|Yhat-Yhat'|): 0.1099\n",
      "[Epoch 100 (50%)] energy-loss: 0.3242,  E(|Y-Yhat|): 0.6631,  E(|Yhat-Yhat'|): 0.6776\n",
      "[Epoch 200 (100%)] energy-loss: 0.3331,  E(|Y-Yhat|): 0.6783,  E(|Yhat-Yhat'|): 0.6904\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3350,  E(|Y-Yhat|): 0.6756,  E(|Yhat-Yhat'|): 0.6811\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8099, -0.3070])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7624,  E(|Y-Yhat|): 0.7953,  E(|Yhat-Yhat'|): 0.0659\n",
      "[Epoch 100 (50%)] energy-loss: 0.3329,  E(|Y-Yhat|): 0.6680,  E(|Yhat-Yhat'|): 0.6701\n",
      "[Epoch 200 (100%)] energy-loss: 0.3313,  E(|Y-Yhat|): 0.6587,  E(|Yhat-Yhat'|): 0.6548\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3271,  E(|Y-Yhat|): 0.6579,  E(|Yhat-Yhat'|): 0.6616\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7726, -0.2829])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0799,  E(|Y-Yhat|): 0.1930,  E(|Yhat-Yhat'|): 0.2261\n",
      "[Epoch 100 (50%)] energy-loss: 0.0217,  E(|Y-Yhat|): 0.0422,  E(|Yhat-Yhat'|): 0.0410\n",
      "[Epoch 200 (100%)] energy-loss: 0.0206,  E(|Y-Yhat|): 0.0399,  E(|Yhat-Yhat'|): 0.0387\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0210,  E(|Y-Yhat|): 0.0404,  E(|Yhat-Yhat'|): 0.0387\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7034, -0.1900])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0304,  E(|Y-Yhat|): 0.0961,  E(|Yhat-Yhat'|): 0.1313\n",
      "[Epoch 100 (50%)] energy-loss: 0.0162,  E(|Y-Yhat|): 0.0310,  E(|Yhat-Yhat'|): 0.0296\n",
      "[Epoch 200 (100%)] energy-loss: 0.0162,  E(|Y-Yhat|): 0.0339,  E(|Yhat-Yhat'|): 0.0356\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0161,  E(|Y-Yhat|): 0.0328,  E(|Yhat-Yhat'|): 0.0333\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7169, -0.3534])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0122,  E(|Y-Yhat|): 0.0466,  E(|Yhat-Yhat'|): 0.0689\n",
      "[Epoch 100 (50%)] energy-loss: 0.0097,  E(|Y-Yhat|): 0.0220,  E(|Yhat-Yhat'|): 0.0246\n",
      "[Epoch 200 (100%)] energy-loss: 0.0104,  E(|Y-Yhat|): 0.0207,  E(|Yhat-Yhat'|): 0.0206\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0099,  E(|Y-Yhat|): 0.0218,  E(|Yhat-Yhat'|): 0.0240\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6746, -0.2714])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0224,  E(|Y-Yhat|): 0.0774,  E(|Yhat-Yhat'|): 0.1101\n",
      "[Epoch 100 (50%)] energy-loss: 0.0164,  E(|Y-Yhat|): 0.0164,  E(|Yhat-Yhat'|): 0.0001\n",
      "[Epoch 200 (100%)] energy-loss: 0.0134,  E(|Y-Yhat|): 0.0258,  E(|Yhat-Yhat'|): 0.0247\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0133,  E(|Y-Yhat|): 0.0320,  E(|Yhat-Yhat'|): 0.0374\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6014, -0.2378])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1849,  E(|Y-Yhat|): 0.3484,  E(|Yhat-Yhat'|): 0.3269\n",
      "[Epoch 100 (50%)] energy-loss: 0.0146,  E(|Y-Yhat|): 0.0146,  E(|Yhat-Yhat'|): 0.0000\n",
      "[Epoch 200 (100%)] energy-loss: 0.0146,  E(|Y-Yhat|): 0.0146,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0146,  E(|Y-Yhat|): 0.0146,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5766, -0.2059])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1554,  E(|Y-Yhat|): 0.2943,  E(|Yhat-Yhat'|): 0.2779\n",
      "[Epoch 100 (50%)] energy-loss: 0.0161,  E(|Y-Yhat|): 0.0161,  E(|Yhat-Yhat'|): 0.0000\n",
      "[Epoch 200 (100%)] energy-loss: 0.0161,  E(|Y-Yhat|): 0.0161,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0161,  E(|Y-Yhat|): 0.0162,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5314, -0.1632])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1104,  E(|Y-Yhat|): 0.2139,  E(|Yhat-Yhat'|): 0.2068\n",
      "[Epoch 100 (50%)] energy-loss: 0.0184,  E(|Y-Yhat|): 0.0184,  E(|Yhat-Yhat'|): 0.0000\n",
      "[Epoch 200 (100%)] energy-loss: 0.0184,  E(|Y-Yhat|): 0.0184,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0184,  E(|Y-Yhat|): 0.0184,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4837, -0.1106])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1803,  E(|Y-Yhat|): 0.3366,  E(|Yhat-Yhat'|): 0.3126\n",
      "[Epoch 100 (50%)] energy-loss: 0.0186,  E(|Y-Yhat|): 0.0186,  E(|Yhat-Yhat'|): 0.0000\n",
      "[Epoch 200 (100%)] energy-loss: 0.0186,  E(|Y-Yhat|): 0.0187,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0186,  E(|Y-Yhat|): 0.0187,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4343, -0.0867])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0920,  E(|Y-Yhat|): 0.2224,  E(|Yhat-Yhat'|): 0.2608\n",
      "[Epoch 100 (50%)] energy-loss: 0.0164,  E(|Y-Yhat|): 0.0378,  E(|Yhat-Yhat'|): 0.0427\n",
      "[Epoch 200 (100%)] energy-loss: 0.0145,  E(|Y-Yhat|): 0.0298,  E(|Yhat-Yhat'|): 0.0305\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0147,  E(|Y-Yhat|): 0.0300,  E(|Yhat-Yhat'|): 0.0306\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4060, -0.0107])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0216,  E(|Y-Yhat|): 0.0538,  E(|Yhat-Yhat'|): 0.0645\n",
      "[Epoch 100 (50%)] energy-loss: 0.0182,  E(|Y-Yhat|): 0.0335,  E(|Yhat-Yhat'|): 0.0305\n",
      "[Epoch 200 (100%)] energy-loss: 0.0182,  E(|Y-Yhat|): 0.0356,  E(|Yhat-Yhat'|): 0.0348\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0181,  E(|Y-Yhat|): 0.0359,  E(|Yhat-Yhat'|): 0.0356\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.3667, 0.1266])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0342,  E(|Y-Yhat|): 0.0645,  E(|Yhat-Yhat'|): 0.0605\n",
      "[Epoch 100 (50%)] energy-loss: 0.0286,  E(|Y-Yhat|): 0.0568,  E(|Yhat-Yhat'|): 0.0565\n",
      "[Epoch 200 (100%)] energy-loss: 0.0281,  E(|Y-Yhat|): 0.0565,  E(|Yhat-Yhat'|): 0.0569\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0284,  E(|Y-Yhat|): 0.0592,  E(|Yhat-Yhat'|): 0.0615\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4773, 0.0581])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0329,  E(|Y-Yhat|): 0.0865,  E(|Yhat-Yhat'|): 0.1073\n",
      "[Epoch 100 (50%)] energy-loss: 0.0291,  E(|Y-Yhat|): 0.0547,  E(|Yhat-Yhat'|): 0.0512\n",
      "[Epoch 200 (100%)] energy-loss: 0.0288,  E(|Y-Yhat|): 0.0595,  E(|Yhat-Yhat'|): 0.0613\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0297,  E(|Y-Yhat|): 0.0575,  E(|Yhat-Yhat'|): 0.0556\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4973, 0.0086])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1233,  E(|Y-Yhat|): 0.2717,  E(|Yhat-Yhat'|): 0.2968\n",
      "[Epoch 100 (50%)] energy-loss: 0.0348,  E(|Y-Yhat|): 0.0665,  E(|Yhat-Yhat'|): 0.0636\n",
      "[Epoch 200 (100%)] energy-loss: 0.0348,  E(|Y-Yhat|): 0.0712,  E(|Yhat-Yhat'|): 0.0728\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0352,  E(|Y-Yhat|): 0.0675,  E(|Yhat-Yhat'|): 0.0647\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5239, -0.0673])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0446,  E(|Y-Yhat|): 0.0969,  E(|Yhat-Yhat'|): 0.1044\n",
      "[Epoch 100 (50%)] energy-loss: 0.0359,  E(|Y-Yhat|): 0.0688,  E(|Yhat-Yhat'|): 0.0657\n",
      "[Epoch 200 (100%)] energy-loss: 0.0358,  E(|Y-Yhat|): 0.0711,  E(|Yhat-Yhat'|): 0.0705\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0366,  E(|Y-Yhat|): 0.0752,  E(|Yhat-Yhat'|): 0.0771\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6370, -0.0389])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0555,  E(|Y-Yhat|): 0.1285,  E(|Yhat-Yhat'|): 0.1461\n",
      "[Epoch 100 (50%)] energy-loss: 0.0450,  E(|Y-Yhat|): 0.0877,  E(|Yhat-Yhat'|): 0.0855\n",
      "[Epoch 200 (100%)] energy-loss: 0.0438,  E(|Y-Yhat|): 0.0903,  E(|Yhat-Yhat'|): 0.0929\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0449,  E(|Y-Yhat|): 0.0945,  E(|Yhat-Yhat'|): 0.0993\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.7570, 0.0115])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1365,  E(|Y-Yhat|): 0.2965,  E(|Yhat-Yhat'|): 0.3199\n",
      "[Epoch 100 (50%)] energy-loss: 0.0492,  E(|Y-Yhat|): 0.1014,  E(|Yhat-Yhat'|): 0.1045\n",
      "[Epoch 200 (100%)] energy-loss: 0.0487,  E(|Y-Yhat|): 0.0975,  E(|Yhat-Yhat'|): 0.0977\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0495,  E(|Y-Yhat|): 0.0988,  E(|Yhat-Yhat'|): 0.0987\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6070, -0.0198])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0646,  E(|Y-Yhat|): 0.1449,  E(|Yhat-Yhat'|): 0.1605\n",
      "[Epoch 100 (50%)] energy-loss: 0.0582,  E(|Y-Yhat|): 0.1177,  E(|Yhat-Yhat'|): 0.1189\n",
      "[Epoch 200 (100%)] energy-loss: 0.0588,  E(|Y-Yhat|): 0.1181,  E(|Yhat-Yhat'|): 0.1187\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0583,  E(|Y-Yhat|): 0.1174,  E(|Yhat-Yhat'|): 0.1182\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.5414, 0.0280])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0817,  E(|Y-Yhat|): 0.1900,  E(|Yhat-Yhat'|): 0.2165\n",
      "[Epoch 100 (50%)] energy-loss: 0.0617,  E(|Y-Yhat|): 0.1264,  E(|Yhat-Yhat'|): 0.1293\n",
      "[Epoch 200 (100%)] energy-loss: 0.0626,  E(|Y-Yhat|): 0.1227,  E(|Yhat-Yhat'|): 0.1202\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0628,  E(|Y-Yhat|): 0.1301,  E(|Yhat-Yhat'|): 0.1347\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6054, -0.0899])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0621,  E(|Y-Yhat|): 0.1186,  E(|Yhat-Yhat'|): 0.1129\n",
      "[Epoch 100 (50%)] energy-loss: 0.0550,  E(|Y-Yhat|): 0.1099,  E(|Yhat-Yhat'|): 0.1099\n",
      "[Epoch 200 (100%)] energy-loss: 0.0529,  E(|Y-Yhat|): 0.1080,  E(|Yhat-Yhat'|): 0.1103\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0530,  E(|Y-Yhat|): 0.1079,  E(|Yhat-Yhat'|): 0.1098\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5349, -0.0345])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0579,  E(|Y-Yhat|): 0.0913,  E(|Yhat-Yhat'|): 0.0670\n",
      "[Epoch 100 (50%)] energy-loss: 0.0453,  E(|Y-Yhat|): 0.0896,  E(|Yhat-Yhat'|): 0.0885\n",
      "[Epoch 200 (100%)] energy-loss: 0.0460,  E(|Y-Yhat|): 0.0922,  E(|Yhat-Yhat'|): 0.0925\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0460,  E(|Y-Yhat|): 0.0954,  E(|Yhat-Yhat'|): 0.0987\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4320, 0.0164])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0766,  E(|Y-Yhat|): 0.1917,  E(|Yhat-Yhat'|): 0.2302\n",
      "[Epoch 100 (50%)] energy-loss: 0.0473,  E(|Y-Yhat|): 0.0918,  E(|Yhat-Yhat'|): 0.0890\n",
      "[Epoch 200 (100%)] energy-loss: 0.0472,  E(|Y-Yhat|): 0.0951,  E(|Yhat-Yhat'|): 0.0957\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0482,  E(|Y-Yhat|): 0.0966,  E(|Yhat-Yhat'|): 0.0968\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4122, 0.1438])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0491,  E(|Y-Yhat|): 0.1157,  E(|Yhat-Yhat'|): 0.1331\n",
      "[Epoch 100 (50%)] energy-loss: 0.0448,  E(|Y-Yhat|): 0.0944,  E(|Yhat-Yhat'|): 0.0992\n",
      "[Epoch 200 (100%)] energy-loss: 0.0464,  E(|Y-Yhat|): 0.0916,  E(|Yhat-Yhat'|): 0.0904\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0457,  E(|Y-Yhat|): 0.0950,  E(|Yhat-Yhat'|): 0.0988\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5050, -0.0054])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0418,  E(|Y-Yhat|): 0.0754,  E(|Yhat-Yhat'|): 0.0671\n",
      "[Epoch 100 (50%)] energy-loss: 0.0377,  E(|Y-Yhat|): 0.0784,  E(|Yhat-Yhat'|): 0.0813\n",
      "[Epoch 200 (100%)] energy-loss: 0.0378,  E(|Y-Yhat|): 0.0767,  E(|Yhat-Yhat'|): 0.0779\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0374,  E(|Y-Yhat|): 0.0747,  E(|Yhat-Yhat'|): 0.0745\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4923, 0.0359])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0478,  E(|Y-Yhat|): 0.1032,  E(|Yhat-Yhat'|): 0.1109\n",
      "[Epoch 100 (50%)] energy-loss: 0.0409,  E(|Y-Yhat|): 0.0858,  E(|Yhat-Yhat'|): 0.0896\n",
      "[Epoch 200 (100%)] energy-loss: 0.0415,  E(|Y-Yhat|): 0.0850,  E(|Yhat-Yhat'|): 0.0871\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0405,  E(|Y-Yhat|): 0.0842,  E(|Yhat-Yhat'|): 0.0875\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4210, -0.0755])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1697,  E(|Y-Yhat|): 0.3346,  E(|Yhat-Yhat'|): 0.3298\n",
      "[Epoch 100 (50%)] energy-loss: 0.0366,  E(|Y-Yhat|): 0.0775,  E(|Yhat-Yhat'|): 0.0818\n",
      "[Epoch 200 (100%)] energy-loss: 0.0364,  E(|Y-Yhat|): 0.0766,  E(|Yhat-Yhat'|): 0.0805\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0369,  E(|Y-Yhat|): 0.0730,  E(|Yhat-Yhat'|): 0.0723\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4366, 0.0184])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1426,  E(|Y-Yhat|): 0.2833,  E(|Yhat-Yhat'|): 0.2815\n",
      "[Epoch 100 (50%)] energy-loss: 0.0389,  E(|Y-Yhat|): 0.0774,  E(|Yhat-Yhat'|): 0.0770\n",
      "[Epoch 200 (100%)] energy-loss: 0.0394,  E(|Y-Yhat|): 0.0778,  E(|Yhat-Yhat'|): 0.0768\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0392,  E(|Y-Yhat|): 0.0764,  E(|Yhat-Yhat'|): 0.0744\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.3275, -0.0343])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1103,  E(|Y-Yhat|): 0.2153,  E(|Yhat-Yhat'|): 0.2099\n",
      "[Epoch 100 (50%)] energy-loss: 0.0403,  E(|Y-Yhat|): 0.0738,  E(|Yhat-Yhat'|): 0.0670\n",
      "[Epoch 200 (100%)] energy-loss: 0.0401,  E(|Y-Yhat|): 0.0810,  E(|Yhat-Yhat'|): 0.0818\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0400,  E(|Y-Yhat|): 0.0811,  E(|Yhat-Yhat'|): 0.0822\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4247, -0.1209])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1591,  E(|Y-Yhat|): 0.3157,  E(|Yhat-Yhat'|): 0.3133\n",
      "[Epoch 100 (50%)] energy-loss: 0.0428,  E(|Y-Yhat|): 0.0873,  E(|Yhat-Yhat'|): 0.0889\n",
      "[Epoch 200 (100%)] energy-loss: 0.0421,  E(|Y-Yhat|): 0.0857,  E(|Yhat-Yhat'|): 0.0872\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0427,  E(|Y-Yhat|): 0.0879,  E(|Yhat-Yhat'|): 0.0903\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4275, -0.1969])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0932,  E(|Y-Yhat|): 0.2239,  E(|Yhat-Yhat'|): 0.2615\n",
      "[Epoch 100 (50%)] energy-loss: 0.0426,  E(|Y-Yhat|): 0.0844,  E(|Yhat-Yhat'|): 0.0837\n",
      "[Epoch 200 (100%)] energy-loss: 0.0415,  E(|Y-Yhat|): 0.0890,  E(|Yhat-Yhat'|): 0.0952\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0432,  E(|Y-Yhat|): 0.0884,  E(|Yhat-Yhat'|): 0.0903\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4182, -0.0571])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0692,  E(|Y-Yhat|): 0.1012,  E(|Yhat-Yhat'|): 0.0639\n",
      "[Epoch 100 (50%)] energy-loss: 0.0520,  E(|Y-Yhat|): 0.1105,  E(|Yhat-Yhat'|): 0.1170\n",
      "[Epoch 200 (100%)] energy-loss: 0.0520,  E(|Y-Yhat|): 0.1033,  E(|Yhat-Yhat'|): 0.1027\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0522,  E(|Y-Yhat|): 0.1053,  E(|Yhat-Yhat'|): 0.1062\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.3145, 0.0566])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0766,  E(|Y-Yhat|): 0.1069,  E(|Yhat-Yhat'|): 0.0607\n",
      "[Epoch 100 (50%)] energy-loss: 0.0565,  E(|Y-Yhat|): 0.1179,  E(|Yhat-Yhat'|): 0.1229\n",
      "[Epoch 200 (100%)] energy-loss: 0.0561,  E(|Y-Yhat|): 0.1191,  E(|Yhat-Yhat'|): 0.1259\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0556,  E(|Y-Yhat|): 0.1126,  E(|Yhat-Yhat'|): 0.1140\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4642, -0.0951])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0675,  E(|Y-Yhat|): 0.1210,  E(|Yhat-Yhat'|): 0.1070\n",
      "[Epoch 100 (50%)] energy-loss: 0.0597,  E(|Y-Yhat|): 0.1176,  E(|Yhat-Yhat'|): 0.1157\n",
      "[Epoch 200 (100%)] energy-loss: 0.0590,  E(|Y-Yhat|): 0.1177,  E(|Yhat-Yhat'|): 0.1173\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0588,  E(|Y-Yhat|): 0.1217,  E(|Yhat-Yhat'|): 0.1259\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4767, -0.0508])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1131,  E(|Y-Yhat|): 0.2605,  E(|Yhat-Yhat'|): 0.2949\n",
      "[Epoch 100 (50%)] energy-loss: 0.0639,  E(|Y-Yhat|): 0.1289,  E(|Yhat-Yhat'|): 0.1301\n",
      "[Epoch 200 (100%)] energy-loss: 0.0633,  E(|Y-Yhat|): 0.1330,  E(|Yhat-Yhat'|): 0.1394\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0645,  E(|Y-Yhat|): 0.1346,  E(|Yhat-Yhat'|): 0.1401\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4853, -0.1177])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0695,  E(|Y-Yhat|): 0.1216,  E(|Yhat-Yhat'|): 0.1044\n",
      "[Epoch 100 (50%)] energy-loss: 0.0558,  E(|Y-Yhat|): 0.1142,  E(|Yhat-Yhat'|): 0.1168\n",
      "[Epoch 200 (100%)] energy-loss: 0.0566,  E(|Y-Yhat|): 0.1107,  E(|Yhat-Yhat'|): 0.1082\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0545,  E(|Y-Yhat|): 0.1045,  E(|Yhat-Yhat'|): 0.1000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5589, -0.1549])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0751,  E(|Y-Yhat|): 0.1483,  E(|Yhat-Yhat'|): 0.1465\n",
      "[Epoch 100 (50%)] energy-loss: 0.0644,  E(|Y-Yhat|): 0.1353,  E(|Yhat-Yhat'|): 0.1417\n",
      "[Epoch 200 (100%)] energy-loss: 0.0628,  E(|Y-Yhat|): 0.1329,  E(|Yhat-Yhat'|): 0.1403\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0653,  E(|Y-Yhat|): 0.1390,  E(|Yhat-Yhat'|): 0.1473\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6603, -0.1467])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1311,  E(|Y-Yhat|): 0.2912,  E(|Yhat-Yhat'|): 0.3201\n",
      "[Epoch 100 (50%)] energy-loss: 0.0701,  E(|Y-Yhat|): 0.1443,  E(|Yhat-Yhat'|): 0.1484\n",
      "[Epoch 200 (100%)] energy-loss: 0.0710,  E(|Y-Yhat|): 0.1435,  E(|Yhat-Yhat'|): 0.1450\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0721,  E(|Y-Yhat|): 0.1441,  E(|Yhat-Yhat'|): 0.1440\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4904, -0.2046])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0886,  E(|Y-Yhat|): 0.1693,  E(|Yhat-Yhat'|): 0.1614\n",
      "[Epoch 100 (50%)] energy-loss: 0.0784,  E(|Y-Yhat|): 0.1601,  E(|Yhat-Yhat'|): 0.1633\n",
      "[Epoch 200 (100%)] energy-loss: 0.0800,  E(|Y-Yhat|): 0.1610,  E(|Yhat-Yhat'|): 0.1621\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0796,  E(|Y-Yhat|): 0.1534,  E(|Yhat-Yhat'|): 0.1477\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4254, -0.1854])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1001,  E(|Y-Yhat|): 0.2085,  E(|Yhat-Yhat'|): 0.2167\n",
      "[Epoch 100 (50%)] energy-loss: 0.0863,  E(|Y-Yhat|): 0.1663,  E(|Yhat-Yhat'|): 0.1600\n",
      "[Epoch 200 (100%)] energy-loss: 0.0843,  E(|Y-Yhat|): 0.1693,  E(|Yhat-Yhat'|): 0.1702\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0840,  E(|Y-Yhat|): 0.1718,  E(|Yhat-Yhat'|): 0.1756\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5500, -0.3514])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0965,  E(|Y-Yhat|): 0.1533,  E(|Yhat-Yhat'|): 0.1137\n",
      "[Epoch 100 (50%)] energy-loss: 0.0806,  E(|Y-Yhat|): 0.1614,  E(|Yhat-Yhat'|): 0.1617\n",
      "[Epoch 200 (100%)] energy-loss: 0.0782,  E(|Y-Yhat|): 0.1568,  E(|Yhat-Yhat'|): 0.1570\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0785,  E(|Y-Yhat|): 0.1614,  E(|Yhat-Yhat'|): 0.1658\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4430, -0.2520])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0852,  E(|Y-Yhat|): 0.1186,  E(|Yhat-Yhat'|): 0.0668\n",
      "[Epoch 100 (50%)] energy-loss: 0.0650,  E(|Y-Yhat|): 0.1373,  E(|Yhat-Yhat'|): 0.1445\n",
      "[Epoch 200 (100%)] energy-loss: 0.0640,  E(|Y-Yhat|): 0.1243,  E(|Yhat-Yhat'|): 0.1207\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0635,  E(|Y-Yhat|): 0.1254,  E(|Yhat-Yhat'|): 0.1237\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4194, -0.2088])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1170,  E(|Y-Yhat|): 0.2322,  E(|Yhat-Yhat'|): 0.2303\n",
      "[Epoch 100 (50%)] energy-loss: 0.0980,  E(|Y-Yhat|): 0.1791,  E(|Yhat-Yhat'|): 0.1622\n",
      "[Epoch 200 (100%)] energy-loss: 0.0975,  E(|Y-Yhat|): 0.1848,  E(|Yhat-Yhat'|): 0.1745\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0977,  E(|Y-Yhat|): 0.1858,  E(|Yhat-Yhat'|): 0.1761\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.3507, -0.0821])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1182,  E(|Y-Yhat|): 0.1845,  E(|Yhat-Yhat'|): 0.1327\n",
      "[Epoch 100 (50%)] energy-loss: 0.1108,  E(|Y-Yhat|): 0.2136,  E(|Yhat-Yhat'|): 0.2056\n",
      "[Epoch 200 (100%)] energy-loss: 0.1098,  E(|Y-Yhat|): 0.2099,  E(|Yhat-Yhat'|): 0.2001\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1093,  E(|Y-Yhat|): 0.2145,  E(|Yhat-Yhat'|): 0.2104\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5150, -0.1733])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0842,  E(|Y-Yhat|): 0.1178,  E(|Yhat-Yhat'|): 0.0672\n",
      "[Epoch 100 (50%)] energy-loss: 0.0810,  E(|Y-Yhat|): 0.1420,  E(|Yhat-Yhat'|): 0.1221\n",
      "[Epoch 200 (100%)] energy-loss: 0.0802,  E(|Y-Yhat|): 0.1327,  E(|Yhat-Yhat'|): 0.1050\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0789,  E(|Y-Yhat|): 0.1287,  E(|Yhat-Yhat'|): 0.0997\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4382, -0.1171])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.0955,  E(|Y-Yhat|): 0.1507,  E(|Yhat-Yhat'|): 0.1104\n",
      "[Epoch 100 (50%)] energy-loss: 0.0873,  E(|Y-Yhat|): 0.1501,  E(|Yhat-Yhat'|): 0.1255\n",
      "[Epoch 200 (100%)] energy-loss: 0.0865,  E(|Y-Yhat|): 0.1603,  E(|Yhat-Yhat'|): 0.1475\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0852,  E(|Y-Yhat|): 0.1595,  E(|Yhat-Yhat'|): 0.1487\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.3472, -0.1872])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2055,  E(|Y-Yhat|): 0.3696,  E(|Yhat-Yhat'|): 0.3284\n",
      "[Epoch 100 (50%)] energy-loss: 0.0795,  E(|Y-Yhat|): 0.1310,  E(|Yhat-Yhat'|): 0.1030\n",
      "[Epoch 200 (100%)] energy-loss: 0.0783,  E(|Y-Yhat|): 0.1331,  E(|Yhat-Yhat'|): 0.1097\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0792,  E(|Y-Yhat|): 0.1441,  E(|Yhat-Yhat'|): 0.1299\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4186, -0.0683])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1870,  E(|Y-Yhat|): 0.3271,  E(|Yhat-Yhat'|): 0.2803\n",
      "[Epoch 100 (50%)] energy-loss: 0.0840,  E(|Y-Yhat|): 0.1392,  E(|Yhat-Yhat'|): 0.1104\n",
      "[Epoch 200 (100%)] energy-loss: 0.0823,  E(|Y-Yhat|): 0.1462,  E(|Yhat-Yhat'|): 0.1277\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0821,  E(|Y-Yhat|): 0.1527,  E(|Yhat-Yhat'|): 0.1414\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.3151, -0.1486])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1601,  E(|Y-Yhat|): 0.2640,  E(|Yhat-Yhat'|): 0.2078\n",
      "[Epoch 100 (50%)] energy-loss: 0.0807,  E(|Y-Yhat|): 0.1319,  E(|Yhat-Yhat'|): 0.1024\n",
      "[Epoch 200 (100%)] energy-loss: 0.0793,  E(|Y-Yhat|): 0.1320,  E(|Yhat-Yhat'|): 0.1054\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0805,  E(|Y-Yhat|): 0.1359,  E(|Yhat-Yhat'|): 0.1107\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.3708, -0.0953])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1971,  E(|Y-Yhat|): 0.3533,  E(|Yhat-Yhat'|): 0.3123\n",
      "[Epoch 100 (50%)] energy-loss: 0.0801,  E(|Y-Yhat|): 0.1313,  E(|Yhat-Yhat'|): 0.1025\n",
      "[Epoch 200 (100%)] energy-loss: 0.0788,  E(|Y-Yhat|): 0.1291,  E(|Yhat-Yhat'|): 0.1006\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0787,  E(|Y-Yhat|): 0.1325,  E(|Yhat-Yhat'|): 0.1075\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.4280, -0.0886])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1366,  E(|Y-Yhat|): 0.2666,  E(|Yhat-Yhat'|): 0.2599\n",
      "[Epoch 100 (50%)] energy-loss: 0.0873,  E(|Y-Yhat|): 0.1509,  E(|Yhat-Yhat'|): 0.1272\n",
      "[Epoch 200 (100%)] energy-loss: 0.0854,  E(|Y-Yhat|): 0.1561,  E(|Yhat-Yhat'|): 0.1414\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0873,  E(|Y-Yhat|): 0.1590,  E(|Yhat-Yhat'|): 0.1434\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.4281, 0.0049])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1288,  E(|Y-Yhat|): 0.1605,  E(|Yhat-Yhat'|): 0.0634\n",
      "[Epoch 100 (50%)] energy-loss: 0.1044,  E(|Y-Yhat|): 0.1822,  E(|Yhat-Yhat'|): 0.1556\n",
      "[Epoch 200 (100%)] energy-loss: 0.1051,  E(|Y-Yhat|): 0.1925,  E(|Yhat-Yhat'|): 0.1748\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1049,  E(|Y-Yhat|): 0.1928,  E(|Yhat-Yhat'|): 0.1758\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.3735, 0.1321])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1648,  E(|Y-Yhat|): 0.1951,  E(|Yhat-Yhat'|): 0.0607\n",
      "[Epoch 100 (50%)] energy-loss: 0.1280,  E(|Y-Yhat|): 0.2340,  E(|Yhat-Yhat'|): 0.2119\n",
      "[Epoch 200 (100%)] energy-loss: 0.1289,  E(|Y-Yhat|): 0.2394,  E(|Yhat-Yhat'|): 0.2209\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1286,  E(|Y-Yhat|): 0.2370,  E(|Yhat-Yhat'|): 0.2166\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.5403, 0.0079])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1593,  E(|Y-Yhat|): 0.2123,  E(|Yhat-Yhat'|): 0.1061\n",
      "[Epoch 100 (50%)] energy-loss: 0.1348,  E(|Y-Yhat|): 0.2535,  E(|Yhat-Yhat'|): 0.2374\n",
      "[Epoch 200 (100%)] energy-loss: 0.1340,  E(|Y-Yhat|): 0.2633,  E(|Yhat-Yhat'|): 0.2586\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1368,  E(|Y-Yhat|): 0.2758,  E(|Yhat-Yhat'|): 0.2779\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.5516, -0.0632])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1745,  E(|Y-Yhat|): 0.3199,  E(|Yhat-Yhat'|): 0.2908\n",
      "[Epoch 100 (50%)] energy-loss: 0.1538,  E(|Y-Yhat|): 0.3007,  E(|Yhat-Yhat'|): 0.2938\n",
      "[Epoch 200 (100%)] energy-loss: 0.1520,  E(|Y-Yhat|): 0.2990,  E(|Yhat-Yhat'|): 0.2940\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1529,  E(|Y-Yhat|): 0.3084,  E(|Yhat-Yhat'|): 0.3109\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6162, -0.0956])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.1936,  E(|Y-Yhat|): 0.2454,  E(|Yhat-Yhat'|): 0.1037\n",
      "[Epoch 100 (50%)] energy-loss: 0.1490,  E(|Y-Yhat|): 0.2796,  E(|Yhat-Yhat'|): 0.2613\n",
      "[Epoch 200 (100%)] energy-loss: 0.1456,  E(|Y-Yhat|): 0.2776,  E(|Yhat-Yhat'|): 0.2640\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1499,  E(|Y-Yhat|): 0.2897,  E(|Yhat-Yhat'|): 0.2796\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7165, -0.1424])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2111,  E(|Y-Yhat|): 0.2835,  E(|Yhat-Yhat'|): 0.1447\n",
      "[Epoch 100 (50%)] energy-loss: 0.1695,  E(|Y-Yhat|): 0.3218,  E(|Yhat-Yhat'|): 0.3046\n",
      "[Epoch 200 (100%)] energy-loss: 0.1715,  E(|Y-Yhat|): 0.3198,  E(|Yhat-Yhat'|): 0.2966\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1744,  E(|Y-Yhat|): 0.3284,  E(|Yhat-Yhat'|): 0.3080\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7936, -0.0407])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2346,  E(|Y-Yhat|): 0.3914,  E(|Yhat-Yhat'|): 0.3137\n",
      "[Epoch 100 (50%)] energy-loss: 0.2013,  E(|Y-Yhat|): 0.3945,  E(|Yhat-Yhat'|): 0.3865\n",
      "[Epoch 200 (100%)] energy-loss: 0.1954,  E(|Y-Yhat|): 0.3951,  E(|Yhat-Yhat'|): 0.3993\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2043,  E(|Y-Yhat|): 0.4153,  E(|Yhat-Yhat'|): 0.4221\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.6890, 0.0088])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3108,  E(|Y-Yhat|): 0.3901,  E(|Yhat-Yhat'|): 0.1587\n",
      "[Epoch 100 (50%)] energy-loss: 0.2324,  E(|Y-Yhat|): 0.4682,  E(|Yhat-Yhat'|): 0.4716\n",
      "[Epoch 200 (100%)] energy-loss: 0.2338,  E(|Y-Yhat|): 0.4541,  E(|Yhat-Yhat'|): 0.4406\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2337,  E(|Y-Yhat|): 0.4597,  E(|Yhat-Yhat'|): 0.4520\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.6723, 0.0450])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3206,  E(|Y-Yhat|): 0.4263,  E(|Yhat-Yhat'|): 0.2115\n",
      "[Epoch 100 (50%)] energy-loss: 0.2429,  E(|Y-Yhat|): 0.4748,  E(|Yhat-Yhat'|): 0.4638\n",
      "[Epoch 200 (100%)] energy-loss: 0.2468,  E(|Y-Yhat|): 0.4807,  E(|Yhat-Yhat'|): 0.4677\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2407,  E(|Y-Yhat|): 0.4884,  E(|Yhat-Yhat'|): 0.4955\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8040, -0.1329])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3119,  E(|Y-Yhat|): 0.3672,  E(|Yhat-Yhat'|): 0.1106\n",
      "[Epoch 100 (50%)] energy-loss: 0.2226,  E(|Y-Yhat|): 0.4356,  E(|Yhat-Yhat'|): 0.4260\n",
      "[Epoch 200 (100%)] energy-loss: 0.2219,  E(|Y-Yhat|): 0.4293,  E(|Yhat-Yhat'|): 0.4148\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2230,  E(|Y-Yhat|): 0.4394,  E(|Yhat-Yhat'|): 0.4330\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7505, -0.1044])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2745,  E(|Y-Yhat|): 0.3077,  E(|Yhat-Yhat'|): 0.0665\n",
      "[Epoch 100 (50%)] energy-loss: 0.1913,  E(|Y-Yhat|): 0.3648,  E(|Yhat-Yhat'|): 0.3469\n",
      "[Epoch 200 (100%)] energy-loss: 0.1878,  E(|Y-Yhat|): 0.3604,  E(|Yhat-Yhat'|): 0.3452\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1910,  E(|Y-Yhat|): 0.3581,  E(|Yhat-Yhat'|): 0.3343\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6675, -0.0319])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 1/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2385,  E(|Y-Yhat|): 0.3514,  E(|Yhat-Yhat'|): 0.2258\n",
      "[Epoch 100 (50%)] energy-loss: 0.2149,  E(|Y-Yhat|): 0.4271,  E(|Yhat-Yhat'|): 0.4244\n",
      "[Epoch 200 (100%)] energy-loss: 0.2169,  E(|Y-Yhat|): 0.4438,  E(|Yhat-Yhat'|): 0.4539\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2218,  E(|Y-Yhat|): 0.4549,  E(|Yhat-Yhat'|): 0.4662\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.6765, 0.1106])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 2/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2592,  E(|Y-Yhat|): 0.3245,  E(|Yhat-Yhat'|): 0.1305\n",
      "[Epoch 100 (50%)] energy-loss: 0.2144,  E(|Y-Yhat|): 0.4099,  E(|Yhat-Yhat'|): 0.3908\n",
      "[Epoch 200 (100%)] energy-loss: 0.2186,  E(|Y-Yhat|): 0.4236,  E(|Yhat-Yhat'|): 0.4099\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2174,  E(|Y-Yhat|): 0.4209,  E(|Yhat-Yhat'|): 0.4070\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.8213, 0.0548])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 3/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2032,  E(|Y-Yhat|): 0.2374,  E(|Yhat-Yhat'|): 0.0684\n",
      "[Epoch 100 (50%)] energy-loss: 0.1684,  E(|Y-Yhat|): 0.3110,  E(|Yhat-Yhat'|): 0.2853\n",
      "[Epoch 200 (100%)] energy-loss: 0.1692,  E(|Y-Yhat|): 0.3086,  E(|Yhat-Yhat'|): 0.2788\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1707,  E(|Y-Yhat|): 0.3190,  E(|Yhat-Yhat'|): 0.2967\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.8719, 0.1444])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 4/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2602,  E(|Y-Yhat|): 0.3148,  E(|Yhat-Yhat'|): 0.1092\n",
      "[Epoch 100 (50%)] energy-loss: 0.2088,  E(|Y-Yhat|): 0.3846,  E(|Yhat-Yhat'|): 0.3518\n",
      "[Epoch 200 (100%)] energy-loss: 0.2057,  E(|Y-Yhat|): 0.3927,  E(|Yhat-Yhat'|): 0.3739\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2091,  E(|Y-Yhat|): 0.4003,  E(|Yhat-Yhat'|): 0.3823\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7193, -0.0285])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 5/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2723,  E(|Y-Yhat|): 0.4339,  E(|Yhat-Yhat'|): 0.3232\n",
      "[Epoch 100 (50%)] energy-loss: 0.1851,  E(|Y-Yhat|): 0.3320,  E(|Yhat-Yhat'|): 0.2939\n",
      "[Epoch 200 (100%)] energy-loss: 0.1815,  E(|Y-Yhat|): 0.3477,  E(|Yhat-Yhat'|): 0.3324\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1800,  E(|Y-Yhat|): 0.3545,  E(|Yhat-Yhat'|): 0.3490\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([1.0000, 0.7469, 0.0056])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 6/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2655,  E(|Y-Yhat|): 0.4035,  E(|Yhat-Yhat'|): 0.2760\n",
      "[Epoch 100 (50%)] energy-loss: 0.1679,  E(|Y-Yhat|): 0.3106,  E(|Yhat-Yhat'|): 0.2854\n",
      "[Epoch 200 (100%)] energy-loss: 0.1671,  E(|Y-Yhat|): 0.3089,  E(|Yhat-Yhat'|): 0.2837\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1682,  E(|Y-Yhat|): 0.3082,  E(|Yhat-Yhat'|): 0.2802\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.6449, -0.0380])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 7/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2634,  E(|Y-Yhat|): 0.3644,  E(|Yhat-Yhat'|): 0.2020\n",
      "[Epoch 100 (50%)] energy-loss: 0.1576,  E(|Y-Yhat|): 0.2777,  E(|Yhat-Yhat'|): 0.2402\n",
      "[Epoch 200 (100%)] energy-loss: 0.1580,  E(|Y-Yhat|): 0.2884,  E(|Yhat-Yhat'|): 0.2610\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1571,  E(|Y-Yhat|): 0.2939,  E(|Yhat-Yhat'|): 0.2737\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7930, -0.1174])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 8/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2299,  E(|Y-Yhat|): 0.3836,  E(|Yhat-Yhat'|): 0.3075\n",
      "[Epoch 100 (50%)] energy-loss: 0.1637,  E(|Y-Yhat|): 0.2875,  E(|Yhat-Yhat'|): 0.2475\n",
      "[Epoch 200 (100%)] energy-loss: 0.1636,  E(|Y-Yhat|): 0.3007,  E(|Yhat-Yhat'|): 0.2744\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1608,  E(|Y-Yhat|): 0.3046,  E(|Yhat-Yhat'|): 0.2876\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7859, -0.1957])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 9/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2272,  E(|Y-Yhat|): 0.3542,  E(|Yhat-Yhat'|): 0.2539\n",
      "[Epoch 100 (50%)] energy-loss: 0.1744,  E(|Y-Yhat|): 0.3123,  E(|Yhat-Yhat'|): 0.2759\n",
      "[Epoch 200 (100%)] energy-loss: 0.1762,  E(|Y-Yhat|): 0.3232,  E(|Yhat-Yhat'|): 0.2940\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1774,  E(|Y-Yhat|): 0.3403,  E(|Yhat-Yhat'|): 0.3259\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7646, -0.1791])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 10/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2601,  E(|Y-Yhat|): 0.2910,  E(|Yhat-Yhat'|): 0.0618\n",
      "[Epoch 100 (50%)] energy-loss: 0.1941,  E(|Y-Yhat|): 0.3711,  E(|Yhat-Yhat'|): 0.3539\n",
      "[Epoch 200 (100%)] energy-loss: 0.1937,  E(|Y-Yhat|): 0.3691,  E(|Yhat-Yhat'|): 0.3507\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1960,  E(|Y-Yhat|): 0.3795,  E(|Yhat-Yhat'|): 0.3670\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.7123, -0.1192])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 11/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3614,  E(|Y-Yhat|): 0.3915,  E(|Yhat-Yhat'|): 0.0603\n",
      "[Epoch 100 (50%)] energy-loss: 0.2515,  E(|Y-Yhat|): 0.4877,  E(|Yhat-Yhat'|): 0.4725\n",
      "[Epoch 200 (100%)] energy-loss: 0.2517,  E(|Y-Yhat|): 0.4913,  E(|Yhat-Yhat'|): 0.4792\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2475,  E(|Y-Yhat|): 0.4981,  E(|Yhat-Yhat'|): 0.5011\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8815, -0.3578])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 12/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3256,  E(|Y-Yhat|): 0.3781,  E(|Yhat-Yhat'|): 0.1051\n",
      "[Epoch 100 (50%)] energy-loss: 0.2539,  E(|Y-Yhat|): 0.4953,  E(|Yhat-Yhat'|): 0.4827\n",
      "[Epoch 200 (100%)] energy-loss: 0.2562,  E(|Y-Yhat|): 0.4926,  E(|Yhat-Yhat'|): 0.4728\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2544,  E(|Y-Yhat|): 0.4951,  E(|Yhat-Yhat'|): 0.4814\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9702, -0.3671])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 13/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.2819,  E(|Y-Yhat|): 0.4224,  E(|Yhat-Yhat'|): 0.2809\n",
      "[Epoch 100 (50%)] energy-loss: 0.2587,  E(|Y-Yhat|): 0.5100,  E(|Yhat-Yhat'|): 0.5024\n",
      "[Epoch 200 (100%)] energy-loss: 0.2579,  E(|Y-Yhat|): 0.4992,  E(|Yhat-Yhat'|): 0.4826\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2611,  E(|Y-Yhat|): 0.5054,  E(|Yhat-Yhat'|): 0.4887\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0117, -0.4115])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 14/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3499,  E(|Y-Yhat|): 0.4008,  E(|Yhat-Yhat'|): 0.1018\n",
      "[Epoch 100 (50%)] energy-loss: 0.2573,  E(|Y-Yhat|): 0.4789,  E(|Yhat-Yhat'|): 0.4431\n",
      "[Epoch 200 (100%)] energy-loss: 0.2551,  E(|Y-Yhat|): 0.5033,  E(|Yhat-Yhat'|): 0.4964\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2536,  E(|Y-Yhat|): 0.4943,  E(|Yhat-Yhat'|): 0.4815\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.1232, -0.4258])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 15/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3984,  E(|Y-Yhat|): 0.4695,  E(|Yhat-Yhat'|): 0.1423\n",
      "[Epoch 100 (50%)] energy-loss: 0.3048,  E(|Y-Yhat|): 0.5868,  E(|Yhat-Yhat'|): 0.5639\n",
      "[Epoch 200 (100%)] energy-loss: 0.3030,  E(|Y-Yhat|): 0.5792,  E(|Yhat-Yhat'|): 0.5524\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3107,  E(|Y-Yhat|): 0.5811,  E(|Yhat-Yhat'|): 0.5408\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.1347, -0.2592])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 16/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.3840,  E(|Y-Yhat|): 0.5375,  E(|Yhat-Yhat'|): 0.3069\n",
      "[Epoch 100 (50%)] energy-loss: 0.3193,  E(|Y-Yhat|): 0.6395,  E(|Yhat-Yhat'|): 0.6403\n",
      "[Epoch 200 (100%)] energy-loss: 0.3293,  E(|Y-Yhat|): 0.6308,  E(|Yhat-Yhat'|): 0.6029\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3284,  E(|Y-Yhat|): 0.6382,  E(|Yhat-Yhat'|): 0.6197\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9989, -0.1855])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 17/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5844,  E(|Y-Yhat|): 0.6623,  E(|Yhat-Yhat'|): 0.1558\n",
      "[Epoch 100 (50%)] energy-loss: 0.4124,  E(|Y-Yhat|): 0.8309,  E(|Yhat-Yhat'|): 0.8369\n",
      "[Epoch 200 (100%)] energy-loss: 0.4158,  E(|Y-Yhat|): 0.8242,  E(|Yhat-Yhat'|): 0.8167\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4264,  E(|Y-Yhat|): 0.8526,  E(|Yhat-Yhat'|): 0.8526\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.8598, -0.1454])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 18/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5903,  E(|Y-Yhat|): 0.6936,  E(|Yhat-Yhat'|): 0.2065\n",
      "[Epoch 100 (50%)] energy-loss: 0.4208,  E(|Y-Yhat|): 0.8407,  E(|Yhat-Yhat'|): 0.8397\n",
      "[Epoch 200 (100%)] energy-loss: 0.4202,  E(|Y-Yhat|): 0.8435,  E(|Yhat-Yhat'|): 0.8467\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4295,  E(|Y-Yhat|): 0.8814,  E(|Yhat-Yhat'|): 0.9038\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0694, -0.2729])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 19/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5783,  E(|Y-Yhat|): 0.6316,  E(|Yhat-Yhat'|): 0.1065\n",
      "[Epoch 100 (50%)] energy-loss: 0.3915,  E(|Y-Yhat|): 0.7741,  E(|Yhat-Yhat'|): 0.7651\n",
      "[Epoch 200 (100%)] energy-loss: 0.3956,  E(|Y-Yhat|): 0.7802,  E(|Yhat-Yhat'|): 0.7694\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3975,  E(|Y-Yhat|): 0.7948,  E(|Yhat-Yhat'|): 0.7946\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  1.0208, -0.2206])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n",
      "\n",
      "Running simulation 20/20\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.5055,  E(|Y-Yhat|): 0.5388,  E(|Yhat-Yhat'|): 0.0665\n",
      "[Epoch 100 (50%)] energy-loss: 0.3401,  E(|Y-Yhat|): 0.6521,  E(|Yhat-Yhat'|): 0.6240\n",
      "[Epoch 200 (100%)] energy-loss: 0.3420,  E(|Y-Yhat|): 0.6669,  E(|Yhat-Yhat'|): 0.6498\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3424,  E(|Y-Yhat|): 0.6689,  E(|Yhat-Yhat'|): 0.6530\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "nonzero weight values: tensor([ 1.0000,  0.9072, -0.1482])\n",
      "Difference between true weight vector and learned weight vector: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"softplus\"\n",
    "noise_dist = \"gaussian\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results3 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"softplus\"\n",
    "noise_dist = \"uniform\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results4 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"square\"\n",
    "noise_dist = \"gaussian\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results5 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"square\"\n",
    "noise_dist = \"uniform\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results6 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"cubic\"\n",
    "noise_dist = \"gaussian\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results7 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_sim = 20\n",
    "x_min = -0.55\n",
    "x_max = 1.64\n",
    "x_lower = 0\n",
    "x_upper = 0.55\n",
    "true_function = \"cubic\"\n",
    "noise_dist = \"uniform\"\n",
    "noise_correlation = 0\n",
    "num_epochs = 200\n",
    "n_train=10000\n",
    "num_points = 1000\n",
    "noise_std = 0.1\n",
    "lr = 0.005\n",
    "batch_size=5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results8 = run_engression_simulations(\n",
    "    N_sim=N_sim, x_min=x_min, x_max=x_max,\n",
    "    x_lower=x_lower,x_upper=x_upper,\n",
    "    true_function=true_function,\n",
    "    n_train=n_train,\n",
    "    noise_dist=noise_dist,\n",
    "    noise_corr=noise_correlation,\n",
    "    num_epochs=num_epochs,\n",
    "    num_points=num_points,\n",
    "    noise_std = noise_std,\n",
    "    batch_size = batch_size,\n",
    "    device=device,lr=lr\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savez(\"simresults2_engression.npz\", results_log1=results1, results_log2=results2,\n",
    "         results_softplus1=results3,results_softplus2=results4,\n",
    "         results_square1=results5,results_square2=results6,\n",
    "         results_cubic1=results7,results_cubic2=results8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
